<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>hadoop的安装与配置 | 隶笔难书</title>
<meta name=keywords content="Hadoop"><meta name=description content="hadoop的安装与配置"><meta name=author content><link rel=canonical href=https://waite.wang/posts/bigdata/hadoop-install-and-config/><link crossorigin=anonymous href=/assets/css/stylesheet.1f25c954b6964bfd120383618e0c76f5d98627118274109d51919206cab78c1e.css integrity="sha256-HyXJVLaWS/0SA4Nhjgx29dmGJxGCdBCdUZGSBsq3jB4=" rel="preload stylesheet" as=style><link rel=icon href=https://waite.wang/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://waite.wang/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://waite.wang/favicon-32x32.png><link rel=apple-touch-icon href=https://waite.wang/apple-touch-icon.png><link rel=mask-icon href=https://waite.wang/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-cn href=https://waite.wang/posts/bigdata/hadoop-install-and-config/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://waite.wang/posts/bigdata/hadoop-install-and-config/"><meta property="og:site_name" content="隶笔难书"><meta property="og:title" content="hadoop的安装与配置"><meta property="og:description" content="hadoop的安装与配置"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-10T16:04:23+08:00"><meta property="article:modified_time" content="2022-12-10T16:04:23+08:00"><meta property="article:tag" content="Hadoop"><meta name=twitter:card content="summary"><meta name=twitter:title content="hadoop的安装与配置"><meta name=twitter:description content="hadoop的安装与配置"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://waite.wang/posts/"},{"@type":"ListItem","position":2,"name":"hadoop的安装与配置","item":"https://waite.wang/posts/bigdata/hadoop-install-and-config/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"hadoop的安装与配置","name":"hadoop的安装与配置","description":"hadoop的安装与配置","keywords":["Hadoop"],"articleBody":"安装JDK 1.创建工作路径\nmkdir /usr/cx 2.解压安装包\ntar -zxvf 安装包位置 -C /usr/cx 3.配置环境变量\nvi ~/.bashrc 在打开的~/.bashrc文件中写入一下内容\n# .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi (----------------注：需要在此处增加内容-------------------) --在这添加-- export JAVA_HOME=/usr/cx/jdk名字版本 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar 4.更新环境变量\nsource ~/.bashrc 5.验证jdk是否配置成功\njava -version 主机名配置 1.编辑主机名\nvi /etc/sysconfig/network 打开后的文件如下\nNETWORKING=yes HOSTNAME=CentOS6.5 -----将此地方更改为localhost ----注意若为本地主机则更改为localhost不是则更改为别的地址 更给后输入reboot重启\nreboot 2.IP地址与主机名映射文件配置\nvi /etc/hosts 打开后的文件如下\n127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 (注：在此行增加内容) ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 在第一行的ip地址后面添加一个localhost ----注意若为本地主机则更改为localhost不是则更改为别的地址 3.检测主机名与IP映射是否配置成功\nping localhost -c 4 SElinux安全配置 1.关闭SElinux\n通过命令使用vi编辑器打开SElinux配置文件\nvi /etc/selinux/config 打开后的文件如下\n# This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=permissive ------ 将这里的值更改为disabled (注：需要更改此行内容) # SELINUXTYPE= can take one of these two values: # targeted - Targeted processes are protected, # mls - Multi Level Security protection. SELINUXTYPE=targeted 2.SElinux配置强制生效\nsetenforce 0 配置SSH免密码登录 1.生成密钥\n输入一下命令生成本机密钥文件\nssh-keygen -t dsa 当出现提示的时候，我们直接按回车即可，默认会将秘钥文件生成到~/.ssh/目录下（由于我们实验所使用的登录用户为root，因此~/.ssh/等同于/root/.ssh/）\n通过一下命令查看~/.ssh目录下的文件\nls ~/.ssh 2.密钥分发\n把当前节点的公钥文件id_dsa.pub内容输出追加到任意节点的~/.ssh/authorized_keys文件的末尾，则在被添加的节点上便可以免密码登录到当前的节点（由于我们是单节点部署，因此直接追加到当前节点的~/.ssh/authorized_keys文件中即可\ncat ~/.ssh/id_dsa.pub \u003e\u003e ~/.ssh/authorized_keys 3.验证免密码登录是否配置成功\nssh localhost ----注意若为本地主机则更改为localhost不是则更改为别的地址 第一次登录的时候，会询问呢是否继续连接，输入yes即可\n连接成功后退出连接\nexit 安装Hadoop 1.解压安装文件\ntar -zxvf Hadoop安装包位置 -C /usr/cx 2.配置Hadoop环境变量\nvi ~/.bashrc 打开后的文件如下\n# .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi export JAVA_HOME=/usr/cx/jdk+版本 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar (----------------在此处增加内容-------------------) export HADOOP_HOME=/usr/cx/hadoop+版本 export PATH=$PATH:$HADOOP_HOME/bin:$PATH export PATH=$PATH:$HADOOP_HOME/sbin:$PATH 退出后执行如下命令，更新环境变量\nsource ~/.bashrc 通过下列命令验证Hadoop环境变量是否配置成功\nhadoop 3.编辑Hadoop配置文件\n使用vi命令打开hadoop-env.sh配置文件进行编辑:\nvi /usr/cx/hadoop版本/etc/hadoop/hadoop-env.sh 打开后的文件如下\n# Set Hadoop-specific environment variables here. # The only required environment variable is JAVA_HOME. All others are # optional. When running a distributed configuration it is best to # set JAVA_HOME in this file, so that it is correctly defined on # remote nodes. # The java implementation to use. export JAVA_HOME=${JAVA_HOME} ---更改为 export JAVA_HOME=/usr/cx/jdk+版本 (注：需要对此行内容进行更改，为Hadoop绑定Java运行环境) # The jsvc implementation to use. Jsvc is required to run secure datanodes # that bind to privileged ports to provide authentication of data transfer # protocol. Jsvc is not required if SASL is configured for authentication of # data transfer protocol using non-privileged ports. #export JSVC_HOME=${JSVC_HOME} export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/etc/hadoop\"} 使用vi命令打开core-site.xml配置文件进行编辑\nvi /usr/cx/hadoop+版本/etc/hadoop/core-site.xml 打开的文件内容如下\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e (注：需要在此处进行相关内容配置) /*设置默认的HDFS访问路径*/ fs.defaultFS hdfs://localhost:9000 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /*缓冲区大小：io.file.buffer.size默认是4KB*/ io.file.buffer.size 131072 /*临时文件夹路径设置*/ hadoop.tmp.dir file:/usr/tmp /*设置使用hduser用户可以代理所有主机用户进行任务提交*/ hadoop.proxyuser.hduser.hosts * /*设置使用hduser用户可以代理所有组用户进行任务提交*/ hadoop.proxyuser.hduser.groups * 退出vi编辑器后输入以下vi命令打开yarn-site.xml文件进行配置\nvi /usr/cx/hadoop+版本/etc/hadoop/yarn-site.xml 打开后的文件内容如下\n\u003c?xml version=\"1.0\"?\u003e (注：需要在此处进行相关内容配置) /*设置NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序*/ yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler /*设置客户端与ResourceManager的通信地址*/ yarn.resourcemanager.address localhost:8032 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /*设置ApplicationMaster调度器与ResourceManager的通信地址*/ yarn.resourcemanager.scheduler.address localhost:8030 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /*设置NodeManager与ResourceManager的通信地址*/ yarn.resourcemanager.resource-tracker.address localhost:8031 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /*设置管理员与ResourceManager的通信地址*/ yarn.resourcemanager.admin.address localhost:8033 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /* ResourceManager的Web地址，监控资源调度*/ yarn.resourcemanager.webapp.address localhost:8088 ----注意若为本地主机则更改为localhost不是则更改为别的地址 使用下列命令复制mapred-site.xml.template文件并重命名为mapred-site.xml：\ncp /usr/cx/hadoop+版本/etc/hadoop/mapred-site.xml.template /usr/cx/hadoop+版本/etc/hadoop/mapred-site.xml 使用vi命令打开mapred-site.xml文件进行配置：\nvi /usr/cx/hadoop+版本/etc/hadoop/mapred-site.xml 打开后的文件内容如下\n\u003c?xml version=\"1.0\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e (注：需要在此处进行相关内容配置) /*Hadoop对MapReduce运行框架一共提供了3种实现，在mapred-site.xml中通过\"mapreduce.framework.name\"这个属性来设置为\"classic\"、\"yarn\"或者\"local\"*/ mapreduce.framework.name yarn /*MapReduce JobHistory Server地址*/ mapreduce.jobhistory.address localhost:10020 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /*MapReduce JobHistory Server Web UI访问地址*/ mapreduce.jobhistory.webapp.address localhost:19888 ----注意若为本地主机则更改为localhost不是则更改为别的地址 执行以下命令创建Hadoop的数据存储目录namenode和datanode\nmkdir -p /hdfs/namenode mkdir -p /hdfs/datanode 使用vi命令打开hdfs-site.xml文件进行配置：\nvi /usr/cx/hadoop+版本/etc/hadoop/hdfs-site.xml 打开的文件内容如下\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e (注：需要在此处进行相关内容配置) /*配置SecondaryNameNode地址*/ dfs.namenode.secondary.http-address localhost:9001 ----注意若为本地主机则更改为localhost不是则更改为别的地址 /*配置NameNode的数据存储目录，需要与上文创建的目录相对应*/ dfs.namenode.name.dir file:/hdfs/namenode /*配置DataNode的数据存储目录，需要与上文创建的目录相对应*/ dfs.datanode.data.dir file:/hdfs/datanode /*配置数据块副本数*/ dfs.replication 1 /*将dfs.webhdfs.enabled属性设置为true，否则就不能使用webhdfs的LISTSTATUS、LIST FILESTATUS等需要列出文件、文件夹状态的命令，因为这些信息都是由namenode保存的*/ dfs.webhdfs.enabled true 使用vi命令打开slaves文件进行配置（要与我们前文设置的主机名相互一致，否则将会引起Hadoop相关进程无法正确启动）：\nvi /usr/cx/hadoop-2.7.1/etc/hadoop/slaves 打开的文件内容如下\nlocalhost ----注意若为本地主机则更改为localhost不是则更改为别的地址 若为localhost则不做更改，因为在本地机器而不是在易优云中需要连接到易优云的主机\n** 4.格式化HDFS **\n通过下列命令格式化HDFS文件系统\nhadoop namenode -format Hadoop运行及测试 ** 1.启动Hadoop**\n通过下列命令启动Hadoop：\nstart-all.sh 通过下列命令，查看相应的JVM进程确定Hadoop是否配置及启动成功：\njps ** Web页面测试**\n用浏览器输入网址 http://localhost:8080和 http://localhost50070\n1 、实验目的 通过本节实验的学习，同学们可以掌握Hadoop集群环境部署与配置。本实验完成后，要求学生掌握以下内容：\n掌握集群所有节点之间SSH免密登录配置方式；\n掌握NTP服务配置，实现节点间的时间同步；\n掌握ZooKeeper集群的搭建方式；\n掌握Hadoop集群的搭建配置流程；\n理解Hadoop集群的高可用（HA）原理，并掌握Hadoop集群的高可用（HA）配置方法。\n2、实验原理 需要按照以下流程，在Linux上进行Hadoop集群的安装部署：\n主机名配置：在大型的Hadoop集群中，往往由成百上千个节点组成，如果通过IP地址对不同节点进行管理，那么集群维护的工作量将会十分繁重，因此在工程环境中，常常通过对每个节点设置唯一的主机名，从而实现对节点进行管理。\nSSH（安全外壳协议）免密码登录配置：推荐安装OpenSSH。Hadoop需要通过SSH来启动Slave列表中各台主机的守护进程，因此SSH也是必须安装的。\n安装配置JDK1.7（或更高版本）：Hadoop是用Java编写的程序，Hadoop的编译及MapReduce的运行都需要使用JDK，因此在安装Hadoop前，必须安装JDK1.7或更高版本。\nNTP服务配置：本实验需要在实现Hadoop集群搭建的同时，并进行高可用性（HA）的配置，因此需要通过ZooKeeper来对集群中的节点进行协调，而ZooKeeper需要保证节点间的时钟相互一致，因此需要在集群中配置NTP服务。\nSElinux安全配置：CentOS默认启用了SElinux，在网络服务方面权限要求比较严格，因此需要对SElinux安全配置进行更改。\nZooKeeper集群搭建：高可用性（HA）Hadoop集群的搭建需要依赖于ZooKeeper来对集群中的节点进行协调，因此需要进行ZooKeeper集群搭建。\nHadoop核心配置。Hadoop的稳定运行需要依赖于其核心配置文件，因此当上述准备工作就绪后，我们便需要着重进行配置文件编写来实现Hadoop的可靠运行。\n我们需要在节点1、节点2、节点3中进行高可用Hadoop集群环境的部署。各个节点所部署的服务如下所示：\n节点1 节点2 节点3 NameNode StandBy ResourceManager StandBy DFSZKFailoverController DFSZKFailoverController DataNode DataNode DataNode NodeManager NodeManager NodeManager JournalNode JournalNode JournalNode 1 集群节点基本配置 步骤1. 节点IP地址查询 在节点1、2、3中通过下面的命令查询节点IP地址： ifconfig 命令运行后的返回结果如下所示 (每台虚拟机的IP地址都是不同的，因此需要以实际地址信息为准）：\n[root@CentOS6 ~]# ifconfig eth6 Link encap:Ethernet HWaddr 02:00:1E:79:09:04 inet addr:10.1.1.4 Bcast:10.1.1.255 Mask:255.255.255.0 inet6 addr: fe80::1eff:fe79:904/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:20832 errors:0 dropped:0 overruns:0 frame:0 TX packets:13052 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:31392026 (29.9 MiB) TX bytes:929956 (908.1 KiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:12 errors:0 dropped:0 overruns:0 frame:0 TX packets:12 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:720 (720.0 b) TX bytes:720 (720.0 b) [root@CentOS6 ~]# 需要记录三个节点的IP地址，在后文中我们需要根据此IP地址进行相关操作\n步骤2. 节点主机名配置 需要在节点1、2、3进行下列操作，将三个主机名分别配置为realtime-1，realtime-2，realtime-3\n通过下列命令使用vi编辑器编辑主机名配置文件： vi /etc/sysconfig/network 打开后的文件内容如下所示：\nNETWORKING=yes HOSTNAME=CentOS6.5 (注：需要将此行内容修改为实际的主机名realtime-1、realtime-2、realtime-3) 在文件中进行内容更改，将HOSTNAME字段内容配置成realtime-： HOSTNAME=realtime-1 编辑完成后保存文件并退出vi编辑器\n更改后的文件内容如下所示：\n更改后的内容会在下次系统重启的时候生效，通过下列命令重新启动系统： reboot 步骤3. 节点1、2、3主机名与IP地址映射文件配置 在节点1、2、3中，通过下列命令使用vi编辑器编辑hosts文件： vi /etc/hosts 打开后的文件内容如下所示：\n127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 (注：在此行增加内容) ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 增加节点1、2、3的IP地址与主机名的映射关系、节点间的IP地址与主机名的映射关系、节点间的IP地址与主机名的映射关系，IP地址与主机名之间用空格分隔（主机名填写为前文配置的节点实际主机名称，IP地址需要根据上文中的查询结果来进行填写，并与实际的主机名相对应）： 10.1.1.4 realtime-1 10.1.1.3 realtime-2 10.1.1.206 realtime-3 更改后的文件内容如下所示\n127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.1.1.4 realtime-1 10.1.1.3 realtime-2 10.1.1.206 realtime-3 编辑完成后保存文件并退出vi编辑器\n通过下列命令检测主机名与IP映射是否配置成功： ping realtime-1 -c 2 如果配置成功，则会显示如下结果：\n[root@realtime-1 ~]# ping realtime-1 -c 2 (注：通过此命令向realtime-1节点发送2个报文) PING realtime-1 (10.1.1.4) 56(84) bytes of data. 64 bytes from realtime-1 (10.1.1.4): icmp_seq=1 ttl=64 time=1.98 ms 64 bytes from realtime-1 (10.1.1.4): icmp_seq=2 ttl=64 time=0.341 ms --- realtime-1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 0.341/1.163/1.985/0.822 ms [root@realtime-1 ~]# 通过下列命令检测主机名与IP映射是否配置成功： ping realtime-2 -c 2 如果配置成功，则会显示如下结果：\n[root@realtime-1 ~]# ping realtime-2 -c 2 (注：通过此命令向realtime-2节点发送2个报文) PING realtime-2 (10.1.1.3) 56(84) bytes of data. 64 bytes from realtime-2 (10.1.1.3): icmp_seq=1 ttl=64 time=0.047 ms 64 bytes from realtime-2 (10.1.1.3): icmp_seq=2 ttl=64 time=0.026 ms --- realtime-2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 999ms rtt min/avg/max/mdev = 0.026/0.036/0.047/0.012 ms [root@realtime-1 ~]# 通过下列命令检测主机名与IP映射是否配置成功： ping realtime-3 -c 2 如果配置成功，则会显示如下结果：\n[root@realtime-1 ~]# ping realtime-3 -c 2 (注：通过此命令向realtime-3节点发送2个报文) PING realtime-3 (10.1.1.206) 56(84) bytes of data. 64 bytes from realtime-3 (10.1.1.206): icmp_seq=1 ttl=64 time=1.36 ms 64 bytes from realtime-3 (10.1.1.206): icmp_seq=2 ttl=64 time=0.315 ms --- realtime-3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1002ms rtt min/avg/max/mdev = 0.315/0.841/1.367/0.526 ms [root@realtime-1 ~]# 如果无法进行正常的报文发送，请检查主机名是否配置正确，同时请检查主机名与IP地址映射是否配置正确。\n2 配置SSH免密码登录 步骤1. 节点1、2、3秘钥配置及分发 例如节点1 : 需要在节点1进行下列操作，在节点1中生成秘钥文件，然后将公钥文件分发到节点2和节点3中，实现在节点1可以免密码登录到集群中的其他主机中。\n通过下面的命令生成密钥（使用rsa加密方式）： echo -e \"\\n\"|ssh-keygen -t rsa -N \"\" \u003e/dev/null 2\u003e\u00261 默认情况下会在~/.ssh/文件夹下生成公钥文件id_rsa.pub和私钥文件id_rsa，通过下面的命令对~/.ssh/内容进行查看：\nll ~/.ssh/ 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ll ~/.ssh/ 总用量 8 -rw-------. 1 root root 1675 11月 29 13:42 id_rsa -rw-r--r--. 1 root root 397 11月 29 13:42 id_rsa.pub [root@realtime-1 ~]# 通过下面的命令将公钥文件发送到本机以及其他两个节点，创建root免密钥通道（需要输入密码：111111）： ssh-copy-id -i /root/.ssh/id_rsa.pub root@realtime-1 # 其他的节点需要随之改动root@realtime-2 and root@realtime-3 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@realtime-1 The authenticity of host 'realtime-1 (10.1.1.4)' can't be established. RSA key fingerprint is 9f:3b:30:10:65:46:c9:c3:2b:fb:e5:28:38:39:9c:84. Are you sure you want to continue connecting (yes/no)? yes (注：此处需要输入yes) Warning: Permanently added 'realtime-1,10.1.1.4' (RSA) to the list of known hosts. root@realtime-1's password: （注：此处需要输入root用户密码，为111111） Now try logging into the machine, with \"ssh 'root@realtime-1'\", and check in: .ssh/authorized_keys to make sure we haven't added extra keys that you weren't expecting. [root@realtime-1 ~]# 步骤2. SSH免密码登录测试 集群中各个节点秘钥分发完毕后，可以通过ssh远程登录命令来测试免密码登录是否配置成功。为了操作统一，我们在节点3中进行下面的操作（在其他节点操作所实现的效果也是一样的）\n在节点3中通过下面的命令可以实现免密码远程登录到节点1： ssh realtime-1 #依次运行realtime-2 and realtime-3 命令运行后的返回结果如下所示：\n[root@realtime-3 ~]# ssh realtime-1 Last login: Thu Nov 29 14:08:34 2018 from realtime-3 [root@realtime-1 ~]# 如果从源主机到目的主机的登录过程中，出现需要输入密码的情况，那么需要检查是否已经成功将源主机的公钥文件发送到目的主机中\n3 安装配置JDK1.8 JDK需要在集群3个节点都进行安装，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行\n我们可以在Oracle JDK的官网下载相应版本的JDK，官网地址为: http://www.oracle.com/technetwork/java/javase/downloads/index.html\n步骤1. 创建工作路径 首先需要在终端中输入下列命令，在/usr目录下建立cx工作路径： mkdir /usr/cx 通过下面的命令实现在节点2和节点3的/usr目录下建立cx工作路径： ssh realtime-2 \"mkdir /usr/cx\" ssh realtime-3 \"mkdir /usr/cx\" 步骤2. 解压安装包 我们可以在/usr/software/目录下找到jdk-8u60-linux-x64.tar.gz安装包，通过下列命令将其解压到/usr/cx/目录下： tar -zxvf /usr/software/jdk-8u60-linux-x64.tar.gz -C /usr/cx 命令执行后的输出内容如下所示：\n(-------------------省略------------------------) jdk1.8.0_60/bin/jmc.ini jdk1.8.0_60/bin/jmap jdk1.8.0_60/bin/serialver jdk1.8.0_60/bin/wsgen jdk1.8.0_60/bin/jrunscript jdk1.8.0_60/bin/javah jdk1.8.0_60/bin/javac jdk1.8.0_60/bin/jvisualvm jdk1.8.0_60/bin/jcontrol jdk1.8.0_60/release [root@realtime-1 ~]# 通过下列命令实现在节点2和节点3中将jdk-8u60-linux-x64.tar.gz安装包解压到/usr/cx/目录下： ssh realtime-2 \"tar -zxvf /usr/software/jdk-8u60-linux-x64.tar.gz -C /usr/cx\" ssh realtime-3 \"tar -zxvf /usr/software/jdk-8u60-linux-x64.tar.gz -C /usr/cx\" 步骤3. 配置环境变量 通过下列命令使用vi编辑器打开 ~/.bashrc文件： vi ~/.bashrc 打开的~/.bashrc文件内容如下所示：\n# .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi (----------------注：需要在此处增加内容-------------------) 在文件中写入下列内容： export JAVA_HOME=/usr/cx/jdk1.8.0_60 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar 编辑完成后保存文件并退出vi编辑器。\n通过下面的命令将环境变量配置文件分发到节点2和节点3： scp ~/.bashrc root@realtime-2:~/.bashrc scp ~/.bashrc root@realtime-3:~/.bashrc 命令执行后的输出内容如下所示：\n[root@realtime-1 ~]# scp ~/.bashrc root@realtime-2:~/.bashrc .bashrc 100% 320 0.3KB/s 00:00 [root@realtime-1 ~]# 步骤4. 更新环境变量 执行如下命令，更新环境变量： source ~/.bashrc 执行如下命令，更新节点2和节点3的环境变量： ssh realtime-2 \"source ~/.bashrc\" ssh realtime-3 \"source ~/.bashrc\" 步骤5. 验证JDK是否配置成功 通过下面的命令验证JDK是否安装并配置成功： java -version 如果出现如下JDK版本信息，则说明安装配置成功：\n[root@realtime-1 ~]# java -version java version \"1.8.0_60\" (注：JDK版本号) Java(TM) SE Runtime Environment (build 1.8.0_60-b27) (注：Java运行环境版本号) Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode) [root@realtime-1 ~]# 通过下面的命令验证节点2和节点3的JDK是否安装并配置成功： ssh realtime-2 \"java -version\" ssh realtime-3 \"java -version\" 如果没有正确输出相关版本信息，请检查~/.bashrc文件中的JDK环境变量是否配置正确，同时请确定是否使用source ~/.bashrc命令更新环境变量配置\n4 NTP服务配置 需要在集群的3台节点中都进行NTP服务的配置\n步骤1. NTP服务配置 在节点1、节点2、节点3中通过下面的命令打开NTP配置文件： vi /etc/ntp.conf 打开后的文件内容如下所示：\n（---------------省略----------------） # Permit all access over the loopback interface. This could # be tightened as well, but to do so would effect some of # the administrative functions. restrict 127.0.0.1 restrict -6 ::1 # Hosts on local network are less restricted. #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst （注：注释此行内容） server 1.centos.pool.ntp.org iburst （注：注释此行内容） server 2.centos.pool.ntp.org iburst （注：注释此行内容） server 3.centos.pool.ntp.org iburst （注：注释此行内容） （注：在此处增加内容） #broadcast 192.168.1.255 autokey # broadcast server （---------------省略----------------） 在文件中进行下列内容更改（通过server字段设置本机为NTP Serevr服务器，通过restrict限制realtime-2和realtime-3主机名对应的主机可以同步时间）：\n#server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 127.127.1.0 fudge 127.127.1.0 stratum 10 restrict realtime-2 nomodify notrap restrict realtime-3 nomodify notrap 更改完成后保存文件并退出编辑器\n步骤2. 启动NTP服务 为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行。\n通过下面的命令在节点1中设定NTP服务自启动： chkconfig ntpd on 通过下面的命令在节点1中启动NTP服务： service ntpd start 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# service ntpd start 正在启动 ntpd： [确定] [root@realtime-1 ~]# 通过下面的命令在节点2中设定NTP服务自启动： ssh realtime-2 \"chkconfig ntpd on\" 通过下面的命令在节点2中启动NTP服务： ssh realtime-2 \"service ntpd start\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh realtime-2 \"service ntpd start\" 正在启动 ntpd：[确定] [root@realtime-1 ~]# 通过下面的命令在节点3中设定NTP服务自启动： ssh realtime-3 \"chkconfig ntpd on\" 通过下面的命令在节点3中启动NTP服务： ssh realtime-3 \"service ntpd start\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh realtime-3 \"service ntpd start\" 正在启动 ntpd：[确定] [root@realtime-1 ~]# 如果服务无法正常启动，会出现相关的错误提示信息，只需要根据错误提示进行更改即可。\n步骤3. NTP服务状态查看 为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行。\n通过下面的命令查看节点1中NTP服务的运行状态： ntpstat 命令运行后的返回结果如下所示（由于节点1是作为Server节点，所以其状态会很快变成synchronised，此时说明服务已经正常启动）：\n[root@realtime-1 ~]# ntpstat synchronised to local net at stratum 11 time correct to within 449 ms polling server every 64 s [root@realtime-1 ~]# 通过下面的命令查看节点2和节点三中NTP服务的运行状态： ssh realtime-2 \"ntpstat\" ssh realtime-3 \"ntpstat\" 命令运行后的返回结果如下所示（由于节点2需要同步节点1的时间，因此需要大概15分钟其状态才会由unsynchronised会变成synchronised，当状态变为synchronised时说明服务已经正常启动）：\n[root@realtime-1 ~]# ssh realtime-2 \"ntpstat\" unsynchronised polling server every 64 s [root@realtime-1 ~]# 服务正常启动后的状态如下所示：\n[root@realtime-1 ~]# ssh realtime-3 \"ntpstat\" synchronised to NTP server (10.1.1.4) at stratum 12 time correct to within 25 ms polling server every 64 s [root@realtime-1 ~]# 当3个节点的状态都显示为synchronised时，则表示ntp服务已经启动成功；如果一直显示unsynchronised,可能是配置文件有错误，因此需要检查IP地址是否配置正确。\n同学们不必一直等待，可以先进行下文的实验，然后过后再查看NTP服务状态。\n5 SElinux安全配置 需要在集群3个节点都进行SElinux配置，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行。\n通过下面的命令，关闭节点1、节点2、节点3的SElinux安全设置： /bin/sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config ssh realtime-2 \"/bin/sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config\" ssh realtime-3 \"/bin/sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config\" 6 安装配置ZooKeeper集群 由于我们需要搭建一套具备高可用性的Hadoop集群，因此需要通过ZooKeeper来进行集群中服务的协调。ZooKeeper需要在集群3个节点进行安装配置，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行\n在模板中我们已经将ZooKeeper安装文件zookeeper-3.4.6.tar.gz放到了/usr/software目录下，同学们可以直接使用\n步骤1. 解压安装包 通过下列命令将ZooKeeper安装包解压到/usr/cx目录下： tar -zxvf /usr/software/zookeeper-3.4.6.tar.gz -C /usr/cx 命令运行后的返回结果如下所示：\n(---------------------省略--------------------) zookeeper-3.4.6/recipes/queue/test/org/apache/zookeeper/recipes/queue/DistributedQueueTest.java zookeeper-3.4.6/recipes/queue/build.xml zookeeper-3.4.6/zookeeper-3.4.6.jar zookeeper-3.4.6/lib/ zookeeper-3.4.6/lib/cobertura/ zookeeper-3.4.6/lib/cobertura/README.txt zookeeper-3.4.6/lib/jline-0.9.94.jar zookeeper-3.4.6/lib/log4j-1.2.16.LICENSE.txt zookeeper-3.4.6/lib/slf4j-log4j12-1.6.1.jar zookeeper-3.4.6/lib/jdiff/ zookeeper-3.4.6/lib/jdiff/zookeeper_3.1.1.xml zookeeper-3.4.6/lib/jdiff/zookeeper_3.4.6-SNAPSHOT.xml zookeeper-3.4.6/lib/jdiff/zookeeper_3.4.6.xml zookeeper-3.4.6/lib/slf4j-api-1.6.1.jar zookeeper-3.4.6/lib/log4j-1.2.16.jar zookeeper-3.4.6/lib/netty-3.7.0.Final.jar zookeeper-3.4.6/lib/jline-0.9.94.LICENSE.txt [root@realtime-1 ~]# 解压完成后，我们可以查看解压后的文件夹内容： ls /usr/cx/zookeeper-3.4.6/ 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ls /usr/cx/zookeeper-3.4.6/ bin dist-maven LICENSE.txt src build.xml docs NOTICE.txt zookeeper-3.4.6.jar CHANGES.txt ivysettings.xml README_packaging.txt zookeeper-3.4.6.jar.asc conf ivy.xml README.txt zookeeper-3.4.6.jar.md5 contrib lib recipes zookeeper-3.4.6.jar.sha1 [root@realtime-1 ~]# 步骤2. 数据存储目录创建 通过下面的命令创建ZooKeeper数据存储目录： mkdir -p /home/data 通过下面的命令创建ZooKeeper日志存储目录：\nmkdir -p /home/logs 通过下面的命令在节点2、节点3中创建ZooKeeper数据存储目录： ssh realtime-2 \"mkdir -p /home/data\" ssh realtime-3 \"mkdir -p /home/data\" 通过下面的命令在节点2、节点3中创建ZooKeeper日志存储目录：\nssh realtime-2 \"mkdir -p /home/logs\" ssh realtime-3 \"mkdir -p /home/logs\" 步骤3. 主机myid编号文件创建 通过下面的命令创建myid文件，并设置节点1对应的编号为1（集群启动后会通过此编号来进行主机识别）： echo \"1\" \u003e /home/data/myid 通过下面的命令在节点2中创建myid文件，并设置节点2对应的编号为2（集群启动后会通过此编号来进行主机识别）： ssh realtime-2 \"echo \"2\" \u003e /home/data/myid\" 通过下面的命令在节点3中创建myid文件，并设置节点3对应的编号为3（集群启动后会通过此编号来进行主机识别）： ssh realtime-3 \"echo \"3\" \u003e /home/data/myid\" 步骤4. ZooKeeper配置文件编辑 通过下列命令创建并打开zoo.cfg配置文件： vi /usr/cx/zookeeper-3.4.6/conf/zoo.cfg 在文件中写入下列内容：\ntickTime=2000 dataDir=/home/data clientPort=2181 dataLogDir=/home/logs initLimit=5 syncLimit=2 server.1=realtime-1:2888:3888 server.2=realtime-2:2888:3888 server.3=realtime-3:2888:3888 编辑完成后保存文件并退出vi编辑器。\n在上述配置中，我们设置心跳时间为2000毫秒，设置ZooKeeper在本地保存数据的目录为/home/data，ZooKeeper监听客户端连接的端口为2181,设置所有Follower和Leader进行同步的时间为5s，设置一个Follower和Leader进行同步的时间为2s。同时设定集群中有3台主机，其中realtime-1对应的主机编号为1，Follower与Leader之间交换信息的端口为2888，进行Leader选举的端口为3888；realtime-2对应的主机编号为2，Follower与Leader之间交换信息的端口为2888，进行Leader选举的端口为3888；realtime-3对应的主机编号为3，Follower与Leader之间交换信息的端口为2888，进行Leader选举的端口为3888。\n步骤5. 文件分发 通过下面的命令将节点1的ZooKeeper文件包分发到节点2、节点3中： scp -r /usr/cx/zookeeper-3.4.6 root@realtime-2:/usr/cx/ scp -r /usr/cx/zookeeper-3.4.6 root@realtime-3:/usr/cx/ 命令运行后的返回结果如下所示：\n（----------------------省略------------------------） Makefile.am 100% 74 0.1KB/s 00:00 zkServer.cmd 100% 1084 1.1KB/s 00:00 zkEnv.sh 100% 2696 2.6KB/s 00:00 zkCleanup.sh 100% 1937 1.9KB/s 00:00 zkCli.sh 100% 1534 1.5KB/s 00:00 zkEnv.cmd 100% 1333 1.3KB/s 00:00 zkCli.cmd 100% 1049 1.0KB/s 00:00 README.txt 100% 238 0.2KB/s 00:00 zkServer.sh 100% 5742 5.6KB/s 00:00 NOTICE.txt 100% 170 0.2KB/s 00:00 zookeeper-3.4.6.jar.md5 100% 33 0.0KB/s 00:00 README.txt 100% 1585 1.6KB/s 00:00 CHANGES.txt 100% 79KB 78.9KB/s 00:00 zookeeper-3.4.6.jar.sha1 100% 41 0.0KB/s 00:00 [root@realtime-1 ~]# 步骤6. ZooKeeper环境变量配置 通过下列命令使用vi编辑器打开 ~/.bashrc文件： vi ~/.bashrc 打开的~/.bashrc文件内容如下所示：\n# .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi export JAVA_HOME=/usr/cx/jdk1.8.0_60 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar (----------------注：需要在此处增加内容-------------------) 在文件中写入下列内容： export ZK_HOME=/usr/cx/zookeeper-3.4.6 export PATH=$PATH:$ZK_HOME/bin 编辑完成后保存文件并退出vi编辑器。\n通过下面的命令将环境变量配置文件分发到节点2和节点3： scp ~/.bashrc root@realtime-2:~/.bashrc scp ~/.bashrc root@realtime-3:~/.bashrc 步骤7. 更新环境变量 执行如下命令，更新环境变量： source ~/.bashrc ssh realtime-2 \"source ~/.bashrc\" ssh realtime-3 \"source ~/.bashrc\" 步骤8. 验证环境变量是否配置成功 通过下面的命令验证环境变量是否配置成功： zkServer.sh ssh realtime-2 \"zkServer.sh\" ssh realtime-3 \"zkServer.sh\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# zkServer.sh JMX enabled by default Using config: /usr/cx/zookeeper-3.4.6/bin/../conf/zoo.cfg Usage: /usr/cx/zookeeper-3.4.6/bin/zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd} [root@realtime-1 ~]# 由输出内容可以看出，ZooKeeper环境变量已经配置正确，并且可以正常执行。\n7 ZooKeeper启动及状态查看 步骤1. ZooKeeper启动 通过下面的命令启动ZooKeeper服务： zkServer.sh start ssh realtime-2 \"zkServer.sh start\" ssh realtime-3 \"zkServer.sh start\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# zkServer.sh start JMX enabled by default Using config: /usr/cx/zookeeper-3.4.6/bin/../conf/zoo.cfg Starting zookeeper ... STARTED [root@realtime-1 ~]# 步骤2. ZooKeeper运行状态查看 ZooKeeper运行之后会随机进行follower角色以及leader角色选举，当leader角色节点出现异常后，会从其他节点中选举出新的leader角色。至于具体哪个节点处于leader状态，需要根据实际情况确定，并不是千篇一律的\n通过下面的命令可以查看ZooKeeper运行状态：\nzkServer.sh status ssh realtime-2 \"zkServer.sh status\" ssh realtime-3 \"zkServer.sh status\" 命令运行后的返回结果如下所示（由返回结果的Mode字段可以看出，当前节点是作为follower角色运行的）：\n[root@realtime-1 ~]# zkServer.sh status JMX enabled by default Using config: /usr/cx/zookeeper-3.4.6/bin/../conf/zoo.cfg Mode: follower [root@realtime-1 ~]# 8 安装配置Hadoop集群 Hadoop需要在集群3个节点进行安装配置，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行\n在模板中，我们已经将相应的Hadoop安装包hadoop-2.7.1.tar.gz放到/usr/software/目录下，同学们不需要再次下载，可以直接使用。\n步骤1. 数据存储目录创建 mkdir -p /hdfs/namenode mkdir -p /hdfs/datanode mkdir -p /hdfs/journalnode mkdir -p /var/log/hadoop-yarn ssh realtime-2 \"mkdir -p /hdfs/namenode\" ssh realtime-2 \"mkdir -p /hdfs/datanode\" ssh realtime-2 \"mkdir -p /hdfs/journalnode\" ssh realtime-2 \"mkdir -p /var/log/hadoop-yarn\" ssh realtime-3 \"mkdir -p /hdfs/namenode\" ssh realtime-3 \"mkdir -p /hdfs/datanode\" ssh realtime-3 \"mkdir -p /hdfs/journalnode\" ssh realtime-3 \"mkdir -p /var/log/hadoop-yarn\" 步骤2. 解压安装文件 通过下列命令解压Hadoop安装文件，将文件解压到/usr/cx目录下：\ntar -zxvf /usr/software/hadoop-2.7.1.tar.gz -C /usr/cx 命令执行后的输出内容如下所示：\n(-------------------省略------------------------) hadoop-2.7.1/libexec/hdfs-config.sh hadoop-2.7.1/README.txt hadoop-2.7.1/NOTICE.txt hadoop-2.7.1/lib/ hadoop-2.7.1/lib/native/ hadoop-2.7.1/lib/native/libhadoop.a hadoop-2.7.1/lib/native/libhadoop.so hadoop-2.7.1/lib/native/libhadooppipes.a hadoop-2.7.1/lib/native/libhdfs.so.0.0.0 hadoop-2.7.1/lib/native/libhadooputils.a hadoop-2.7.1/lib/native/libhdfs.a hadoop-2.7.1/lib/native/libhdfs.so hadoop-2.7.1/lib/native/libhadoop.so.1.0.0 hadoop-2.7.1/LICENSE.txt [root@master ~]# 步骤3. 编辑Hadoop配置文件： 使用vi命令打开hadoop-env.sh配置文件进行编辑： vi /usr/cx/hadoop-2.7.1/etc/hadoop/hadoop-env.sh 打开后的文件内容如下所示：\n(-------------------省略------------------------) # Set Hadoop-specific environment variables here. # The only required environment variable is JAVA_HOME. All others are # optional. When running a distributed configuration it is best to # set JAVA_HOME in this file, so that it is correctly defined on # remote nodes. # The java implementation to use. export JAVA_HOME=${JAVA_HOME} (注：需要对此行内容进行更改，为Hadoop绑定Java运行环境) # The jsvc implementation to use. Jsvc is required to run secure datanodes # that bind to privileged ports to provide authentication of data transfer # protocol. Jsvc is not required if SASL is configured for authentication of # data transfer protocol using non-privileged ports. #export JSVC_HOME=${JSVC_HOME} export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/etc/hadoop\"} (-------------------省略------------------------) 在文件中进行下列内容更改，将JAVA_HOME对应的值改成实际的JDK安装路径：\nexport JAVA_HOME=/usr/cx/jdk1.8.0_60\n编辑完成后保存文件并退出vi编辑器。\n使用vi命令打开hdfs-site.xml文件进行配置： vi /usr/cx/hadoop-2.7.1/etc/hadoop/hdfs-site.xml 打开后的文件内容如下所示：\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e (注：需要在此处进行相关内容配置) 在文件中和之间增加下列内容：\n/*配置DataNode的数据存储目录，需要与上文创建的目录相对应*/ dfs.datanode.data.dir /hdfs/datanode /*配置数据块大小为256M*/ dfs.blocksize 268435456 /*自定义的HDFS服务名，在高可用集群中，无法配置单一HDFS服务器入口，所以需要指定一个逻辑上的服务名，当访问服务名时，会自动选择NameNode节点进行访问*/ dfs.nameservices HDFScluster /*配置NameNode的数据存储目录，需要与上文创建的目录相对应*/ dfs.namenode.name.dir /hdfs/namenode /*定义HDFS服务名所指向的NameNode主机名称*/ dfs.ha.namenodes.HDFScluster realtime-1,realtime-2 /*设置NameNode的完整监听地址*/ dfs.namenode.rpc-address.HDFScluster.realtime-1 realtime-1:8020 /*设置NameNode的完整监听地址*/ dfs.namenode.rpc-address.HDFScluster.realtime-2 realtime-2:8020 /*设置NameNode的HTTP访问地址*/ dfs.namenode.http-address.HDFScluster.realtime-1 realtime-1:50070 /*设置NameNode的HTTP访问地址*/ dfs.namenode.http-address.HDFScluster.realtime-2 realtime-2:50070 /*设置主从NameNode元数据同步地址，官方推荐将nameservice作为最后的journal ID*/ dfs.namenode.shared.edits.dir qjournal://realtime-1:8485;realtime-2:8485;realtime-3:8485/HDFScluster /*设置HDFS客户端用来连接集群中活动状态NameNode节点的Java类*/ dfs.client.failover.proxy.provider.HDFScluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider /*设置SSH登录的私钥文件地址*/ dfs.ha.fencing.ssh.private-key-files /root/.ssh/id_rsa /*启动fence过程，确保集群高可用性*/ dfs.ha.fencing.methods shell(/bin/true) /*配置JournalNode的数据存储目录，需要与上文创建的目录相对应*/ dfs.journalnode.edits.dir /hdfs/journalnode /*设置自动切换活跃节点，保证集群高可用性*/ dfs.ha.automatic-failover.enabled true /*配置数据块副本数*/ dfs.replication 3 /*将dfs.webhdfs.enabled属性设置为true，否则就不能使用webhdfs的LISTSTATUS、LIST FILESTATUS等需要列出文件、文件夹状态的命令，因为这些信息都是由namenode保存的*/ dfs.webhdfs.enabled true 编辑完成后保存文件并退出vi编辑器\n在集群中，对HDFS集群访问的入口是NameNode所在的服务器。但是在两个NameNode节点的HA集群中，无法配置单一服务器入口，所以需要通过dfs.nameservices指定一个逻辑上的服务名，这个服务名是自定义的。当外界访问HDFS集群时，入口就变为这个服务名称，Hadoop会自动实现将访问请求转发到实际的处于Active状态的NameNode节点上。\n当配置了HDFS HA集群时，会有两个NameNode，为了避免两个NameNode都为Active状态，当发生failover时，Standby的节点要执行一系列方法把原来那个Active节点中不健康的NameNode服务给杀掉（这个过程就称为fence）。而dfs.ha.fencing.methods配置就是配置了执行杀死原来Active NameNode服务的方法，为了保险起见，因此指定无论如何都把StandBy节点的状态提升为Active，所以最后要配置一个shell(/bin/true)，保证不论前面的方法执行的情况如何，最后fence过程返回的结果都为True。fence操作需要通过SSH进行节点间的访问，因此需要配置dfs.ha.fencing.ssh.private-key-files为所需要用到的私钥文件路径信息。\n使用vi命令打开core-site.xml配置文件进行编辑： vi /usr/cx/hadoop-2.7.1/etc/hadoop/core-site.xml 打开后的文件内容如下所示：\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e (注：需要在此处进行相关内容配置) 在文件中和之间增加下列内容：\n/*设置默认的HDFS访问路径，需要与hdfs-site.xml中的HDFS服务名相一致*/ fs.defaultFS hdfs://HDFScluster /*临时文件夹路径设置*/ hadoop.tmp.dir /usr/tmp /*配置ZooKeeper服务集群，用于活跃NameNode节点的选举*/ ha.zookeeper.quorum realtime-1:2181,realtime-2:2181,realtime-3:2181 /*设置数据压缩算法*/ io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec io.compression.codec.lzo.class com.hadoop.compression.lzo.LzoCodec /*设置使用hduser用户可以代理所有主机用户进行任务提交*/ hadoop.proxyuser.hduser.host * /*设置使用hduser用户可以代理所有组用户进行任务提交*/ hadoop.proxyuser.hduser.groups * 编辑完成后保存文件并退出vi编辑器\n对HDFS集群访问的入口是NameNode所在的服务器，但是在两个NameNode节点的HA集群中，无法配置单一服务器入口，所以需要在hdfs-site.xml中通过dfs.nameservices指定一个逻辑上的服务名，因此此处的fs.defaultFS配置的入口地址需要与hdfs-site.xml中dfs.nameservices所配置的一致。\n使用vi命令打开yarn-site.xml文件进行配置： vi /usr/cx/hadoop-2.7.1/etc/hadoop/yarn-site.xml 打开后的文件内容如下所示：\n\u003c?xml version=\"1.0\"?\u003e (注：需要在此处进行相关内容配置) 在文件中和之间增加下列内容：\n/*设置NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序*/ yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce_shuffle.class org.apache.hadoop.mapred.ShuffleHandler /*设置任务日志存储目录*/ yarn.nodemanager.log-dirs file:///var/log/hadoop-yarn /*设置Hadoop依赖包地址*/ yarn.application.classpath $HADOOP_HOME/share/hadoop/common/*,$HADOOP_HOME/share/hadoop/common/lib/*, $HADOOP_HOME/share/hadoop/hdfs/*,$HADOOP_HOME/share/hadoop/hdfs/lib/*, $HADOOP_HOME/share/hadoop/mapreduce/*,$HADOOP_HOME/share/hadoop/mapreduce/lib/*, $HADOOP_HOME/share/hadoop/yarn/*,$HADOOP_HOME/share/hadoop/yarn/lib/* /*开启resourcemanager 的高可用性功能*/ yarn.resourcemanager.ha.enabled true /*标识集群中的resourcemanager，如果设置该选项，需要确保所有的resourcemanager节点在配置中都有自己的逻辑id*/ yarn.resourcemanager.cluster-id YARNcluster /*设置resourcemanager节点的逻辑id*/ yarn.resourcemanager.ha.rm-ids rm1,rm2 /*为每个逻辑id绑定实际的主机名称*/ yarn.resourcemanager.hostname.rm1 realtime-1 yarn.resourcemanager.hostname.rm2 realtime-2 /*指定ZooKeeper服务地址*/ yarn.resourcemanager.zk-address realtime-1:2181,realtime-2:2181,realtime-3:2181 /*指定resourcemanager的WEB访问地址*/ yarn.resourcemanager.webapp.address.rm1 realtime-1:8089 yarn.resourcemanager.webapp.address.rm2 realtime-2:8089 /*设定虚拟内存与实际内存的比例，比例值越高，则可用虚拟内存就越多*/ yarn.nodemanager.vmem-pmem-ratio 3 /*设定单个容器可以申领到的最小内存资源*/ yarn.scheduler.minimum-allocation-mb 32 /*设置当任务运行结束后，日志文件被转移到的HDFS目录*/ yarn.nodemanager.remote-app-log-dir hdfs://HDFScluster/var/log/hadoop-yarn/apps /*设定资源调度策略，目前可用的有FIFO、Capacity Scheduler和Fair Scheduler*/ yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler /*设定每个任务能够申领到的最大虚拟CPU数目*/ yarn.scheduler.maximum-allocation-vcores 8 /*设置任务完成指定时间（秒）之后，删除任务的本地化文件和日志目录*/ yarn.nodemanager.delete.debug-delay-sec 600 /*设置志在HDFS上保存多长时间（秒）*/ yarn.nodemanager.log.retain-seconds 86400 /*设定物理节点有2G内存加入资源池*/ yarn.nodemanager.resource.memory-mb 2048 编辑完成后保存文件并退出vi编辑器\n在集群中，提交任务的入口是ResourceManager所在的服务器。但是在两个ResourceManager节点的HA集群中，无法配置单一服务器入口，所以需要通过yarn.resourcemanager.cluster-id指定一个逻辑上的服务名，这个服务名是自定义的。当外界向集群提交任务时，入口就变为这个服务名称，YARN会自动实现将访问请求转发到实际的处于Active状态的ResourceManager节点上。由于配置了逻辑服务名，所以需要设置resourcemanager节点的逻辑id，并为每个逻辑id绑定实际的主机名称\n使用下列命令复制mapred-site.xml.template文件并重命名为mapred-site.xml： cp /usr/cx/hadoop-2.7.1/etc/hadoop/mapred-site.xml.template /usr/cx/hadoop-2.7.1/etc/hadoop/mapred-site.xml 使用vi命令打开mapred-site.xml文件进行配置： vi /usr/cx/hadoop-2.7.1/etc/hadoop/mapred-site.xml 打开后的文件内容如下所示：\n\u003c?xml version=\"1.0\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e (注：需要在此处进行相关内容配置) 在文件中和之间增加下列内容：\n/*Hadoop对MapReduce运行框架一共提供了3种实现，在mapred-site.xml中通过\"mapreduce.framework.name\"这个属性来设置为\"classic\"、\"yarn\"或者\"local\"*/ mapreduce.framework.name yarn 编辑完成后保存文件并退出vi编辑器\n使用vi命令打开slaves文件进行配置（要与我们前文设置的主机名相互一致，否则将会引起Hadoop相关进程无法正确启动）： vi /usr/cx/hadoop-2.7.1/etc/hadoop/slaves 打开后的文件内容如下所示：\nlocalhost （注：需要对此内容进行更改，配置为Slave节点的实际主机名） 将文件中的内容更改为下列内容：\nrealtime-1 realtime-2 realtime-3 编辑完成后保存文件并退出vi编辑器\n步骤4. 文件分发 通过下面的命令将节点1的Hadoop文件包分发到节点2中： scp -r /usr/cx/hadoop-2.7.1 root@realtime-2:/usr/cx/ 命令运行后的返回结果如下所示：\n（---------------------省略-----------------------） external.png 100% 230 0.2KB/s 00:00 banner.jpg 100% 872 0.9KB/s 00:00 maven-feather.png 100% 3330 3.3KB/s 00:00 build-by-maven-white.png 100% 2260 2.2KB/s 00:00 build-by-maven-black.png 100% 2294 2.2KB/s 00:00 bg.jpg 100% 486 0.5KB/s 00:00 icon_error_sml.gif 100% 1010 1.0KB/s 00:00 logo_apache.jpg 100% 33KB 32.7KB/s 00:00 collapsed.gif 100% 820 0.8KB/s 00:00 apache-maven-project-2.png 100% 33KB 32.7KB/s 00:00 icon_success_sml.gif 100% 990 1.0KB/s 00:00 icon_info_sml.gif 100% 606 0.6KB/s 00:00 h3.jpg 100% 431 0.4KB/s 00:00 maven-logo-2.gif 100% 26KB 25.8KB/s 00:00 h5.jpg 100% 357 0.4KB/s 00:00 newwindow.png 100% 220 0.2KB/s 00:00 icon_warning_sml.gif 100% 576 0.6KB/s 00:00 expanded.gif 100% 52 0.1KB/s 00:00 dependency-analysis.html 100% 21KB 21.3KB/s 00:00 [root@realtime-1 ~]# 通过下面的命令将节点1的Hadoop文件包分发到节点3中： scp -r /usr/cx/hadoop-2.7.1 root@realtime-3:/usr/cx/ 命令运行后的返回结果如下所示：\n（---------------------省略-----------------------） external.png 100% 230 0.2KB/s 00:00 banner.jpg 100% 872 0.9KB/s 00:00 maven-feather.png 100% 3330 3.3KB/s 00:00 build-by-maven-white.png 100% 2260 2.2KB/s 00:00 build-by-maven-black.png 100% 2294 2.2KB/s 00:00 bg.jpg 100% 486 0.5KB/s 00:00 icon_error_sml.gif 100% 1010 1.0KB/s 00:00 logo_apache.jpg 100% 33KB 32.7KB/s 00:00 collapsed.gif 100% 820 0.8KB/s 00:00 apache-maven-project-2.png 100% 33KB 32.7KB/s 00:00 icon_success_sml.gif 100% 990 1.0KB/s 00:00 icon_info_sml.gif 100% 606 0.6KB/s 00:00 h3.jpg 100% 431 0.4KB/s 00:00 maven-logo-2.gif 100% 26KB 25.8KB/s 00:00 h5.jpg 100% 357 0.4KB/s 00:00 newwindow.png 100% 220 0.2KB/s 00:00 icon_warning_sml.gif 100% 576 0.6KB/s 00:00 expanded.gif 100% 52 0.1KB/s 00:00 dependency-analysis.html 100% 21KB 21.3KB/s 00:00 [root@realtime-1 ~]# 步骤5. 配置Hadoop环境变量 通过下列命令使用vi编辑器编辑~/.bashrc文件： vi ~/.bashrc 打开后的文件内容如下所示：\n# .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi export JAVA_HOME=/usr/cx/jdk1.8.0_60 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar export ZK_HOME=/usr/cx/zookeeper-3.4.6 export PATH=$PATH:$ZK_HOME/bin (----------------在此处增加内容-------------------) 在~/.bashrc文件中增加以下内容： export HADOOP_HOME=/usr/cx/hadoop-2.7.1 export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin 编辑完成后保存文件并退出vi编辑器\n通过下面的命令将节点1的环境变量文件分发到节点2中： scp -r ~/.bashrc root@realtime-2:~/.bashrc 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# scp -r ~/.bashrc root@realtime-2:~/.bashrc .bashrc 100% 502 0.5KB/s 00:00 [root@realtime-1 ~]# 通过下面的命令将节点1的环境变量文件分发到节点3中： scp -r ~/.bashrc root@realtime-3:~/.bashrc 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# scp -r ~/.bashrc root@realtime-3:~/.bashrc .bashrc 100% 502 0.5KB/s 00:00 [root@realtime-1 ~]# 步骤6. 更新环境变量 执行如下命令，更新环境变量： source ~/.bashrc ssh realtime-2 \"source ~/.bashrc\" ssh realtime-3 \"source ~/.bashrc\" 步骤7. 验证Hadoop环境变量是否配置成功 通过下列命令验证Hadoop环境变量是否配置成功： hadoop ssh realtime-2 \"hadoop\" ssh realtime-3 \"hadoop\" 如果出现如下提示信息，则说明Hadoop安装配置成功： [root@realtime-1 ~]# hadoop Usage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters. [root@realtime-1 ~]# 如果没有正确输出相关信息，请检查~/.bashrc文件中的Hadoop环境变量是否配置正确，同时请确定是否使用source ~/.bashrc命令更新环境变量配置\n步骤8. 格式化HDFS 通过下列命令格式化HDFS文件系统（如果格式化失败，会有相关的错误日志输出，根据输出内容进行更改即可）：\nhadoop namenode -format 命令运行后的部分显示内容如下所示：\n(-------------------省略------------------------) 18/11/30 11:07:15 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25 18/11/30 11:07:15 INFO namenode.FSNamesystem: Retry cache on namenode is enabled 18/11/30 11:07:15 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis 18/11/30 11:07:15 INFO util.GSet: Computing capacity for map NameNodeRetryCache 18/11/30 11:07:15 INFO util.GSet: VM type = 64-bit 18/11/30 11:07:15 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB 18/11/30 11:07:15 INFO util.GSet: capacity = 2^15 = 32768 entries 18/11/30 11:07:16 INFO namenode.FSImage: Allocated new BlockPoolId: BP-348760827-10.1.1.4-1543547236332 18/11/30 11:07:16 INFO common.Storage: Storage directory /hdfs/namenode has been successfully formatted. 18/11/30 11:07:16 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid \u003e= 0 18/11/30 11:07:16 INFO util.ExitUtil: Exiting with status 0 18/11/30 11:07:16 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at realtime-1/10.1.1.4 ************************************************************/ [root@realtime-1 ~]# 步骤9. 格式化zkfc元数据 通过下面的命令格式化DFSZKFailoverController(ZKFC)元数据（在一个节点中进行处理即可）：\nhdfs zkfc -formatZK 命令运行后的返回结果如下所示：\n（---------------省略------------------） tString=realtime-1:2181,realtime-2:2181,realtime-3:2181 sessionTimeout=5000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@10e31a9a 18/11/30 11:37:46 INFO zookeeper.ClientCnxn: Opening socket connection to server realtime-2/10.1.1.3:2181. Will not attempt to authenticate using SASL (unknown error) 18/11/30 11:37:47 INFO zookeeper.ClientCnxn: Socket connection established to realtime-2/10.1.1.3:2181, initiating session 18/11/30 11:37:47 INFO zookeeper.ClientCnxn: Session establishment complete on server realtime-2/10.1.1.3:2181, sessionid = 0x2675e9a37a90000, negotiated timeout = 5000 18/11/30 11:37:47 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/HDFScluster in ZK. 18/11/30 11:37:47 INFO ha.ActiveStandbyElector: Session connected. 18/11/30 11:37:47 INFO zookeeper.ZooKeeper: Session: 0x2675e9a37a90000 closed 18/11/30 11:37:47 INFO zookeeper.ClientCnxn: EventThread shut down [root@realtime-1 ~]# 9 Hadoop集群启动运行 我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行\n步骤1. 启动HDFS相关服务 通过下面的命令可以启动HDFS相关服务： start-dfs.sh 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# start-dfs.sh 18/11/30 11:55:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [realtime-2 realtime-1] realtime-2: starting namenode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-namenode-realtime-2.out realtime-1: starting namenode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-namenode-realtime-1.out realtime-1: starting datanode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-datanode-realtime-1.out realtime-2: starting datanode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-datanode-realtime-2.out realtime-3: starting datanode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-datanode-realtime-3.out Starting journal nodes [realtime-1 realtime-2 realtime-3] realtime-1: starting journalnode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-journalnode-realtime-1.out realtime-2: starting journalnode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-journalnode-realtime-2.out realtime-3: starting journalnode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-journalnode-realtime-3.out 18/11/30 11:56:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting ZK Failover Controllers on NN hosts [realtime-2 realtime-1] realtime-2: starting zkfc, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-zkfc-realtime-2.out realtime-1: starting zkfc, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-zkfc-realtime-1.out [root@realtime-1 ~]# 通过下面的命令查看节点1中对应的相关服务： jps ssh realtime-2 \"jps\" ssh realtime-3 \"jps\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# jps 10033 ResourceManager 9427 DataNode 9315 NameNode 2597 QuorumPeerMain 10457 Jps 9625 JournalNode 9818 DFSZKFailoverController 10140 NodeManager 1743 VmServer.jar [root@realtime-1 ~]# 通过下面的命令在节点2中启动ResourceManager进程：\nssh realtime-2 \"yarn-daemon.sh start resourcemanager\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh realtime-2 \"yarn-daemon.sh start resourcemanager\" starting resourcemanager, logging to /usr/cx/hadoop-2.7.1/logs/yarn-root-resourcemanager-realtime-2.out [root@realtime-1 ~]# 通过下面的命令查看节点2中对应的相关服务：\nssh realtime-2 \"jps\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh realtime-2 \"jps\" 5792 DataNode 6164 NameNode 1703 VmServer.jar 6779 ResourceManager 6428 NodeManager 5981 DFSZKFailoverController 6846 Jps 2686 QuorumPeerMain 5887 JournalNode [root@realtime-1 ~]# 由返回结果可以看出，此时在节点2中已经成功启动了ResourceManager进程\n10 Hadoop 高可用性测试 笔者在写作过程中是在节点1中进行下面的操作，同学们可以在任意节点中进行下面的操作，所实现的效果是一致的\n步骤1. NodeManager状态查看 由于设置了2个NameNode，因此必然会有一个处于Active状态，一个处于StandBy状态，至于具体哪个节点处于Active状态，需要根据实际情况确定，并不是千篇一律的。\n当Hadoop成功启动后，我们打开浏览器，输入网址http://realtime-1:50070便可以访问HDFS的Web管理页面（此时可以看到realtime-1节点是处于active状态的）： 输入网址http://realtime-2:50070依然可以访问HDFS的Web管理页面（此时可以看到realtime-2节点是处于standby状态的）： 步骤2. ResourceManager状态查看 由于设置了2个ResourceManager，因此必然会有一个处于Active状态，一个处于StandBy状态，至于具体哪个节点处于Active状态，需要根据实际情况确定，并不是千篇一律的。\n在终端模拟器中，通过下面的命令可以查看逻辑ID为rm1（实际映射的节点为realtime-1）的节点对应的ResourceManager状态： yarn rmadmin -getServiceState rm1 命令运行后的返回结果如下所示（可见当前节点是active状态）：\n[root@realtime-1 ~]# yarn rmadmin -getServiceState rm1 18/11/30 16:35:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable active [root@realtime-1 ~]# 在终端模拟器中，通过下面的命令可以查看逻辑ID为rm2（实际映射的节点为realtime-2）的节点对应的ResourceManager状态： yarn rmadmin -getServiceState rm2 命令运行后的返回结果如下所示（可见当前节点是standby状态）：\n[root@realtime-1 ~]# yarn rmadmin -getServiceState rm2 18/11/30 16:35:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable standby [root@realtime-1 ~]# 步骤3. HDFS高可用测试 通过下面的命令在HDFS中创建测试文件夹/test： hadoop fs -mkdir /test 通过下面的命令查看HDFS中创建的测试文件夹/test： hadoop fs -ls / 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# hadoop fs -ls / 18/11/30 16:40:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Found 1 items drwxr-xr-x - root supergroup 0 2018-11-30 16:39 /test [root@realtime-1 ~]# 由返回结果可以看出，此时依然可以成功查询HDFS文件信息\n打开浏览器，输入网址http://realtime-2:50070访问HDFS的Web管理页面，此时可以看到realtime-2节点已经成功接替成为NameNode并处于active状态（同学们需要根据实际情况来确定）：\n步骤4. YARN高可用测试 通过下面的命令，使用Hadoop自带的案例测试MapReduce应用程序的运行： hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output 18/11/30 16:47:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/11/30 16:47:15 INFO input.FileInputFormat: Total input paths to process : 0 18/11/30 16:47:15 INFO mapreduce.JobSubmitter: number of splits:0 18/11/30 16:47:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1543557487449_0001 18/11/30 16:47:16 INFO impl.YarnClientImpl: Submitted application application_1543557487449_0001 18/11/30 16:47:16 INFO mapreduce.Job: The url to track the job: http://realtime-1:8089/proxy/application_1543557487449_0001/ 18/11/30 16:47:16 INFO mapreduce.Job: Running job: job_1543557487449_0001 18/11/30 16:47:26 INFO mapreduce.Job: Job job_1543557487449_0001 running in uber mode : false 18/11/30 16:47:26 INFO mapreduce.Job: map 0% reduce 0% 18/11/30 16:47:37 INFO mapreduce.Job: map 0% reduce 100% 18/11/30 16:47:38 INFO mapreduce.Job: Job job_1543557487449_0001 completed successfully 18/11/30 16:47:39 INFO mapreduce.Job: Counters: 38 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=119357 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=0 HDFS: Number of bytes written=0 HDFS: Number of read operations=3 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched reduce tasks=1 Total time spent by all maps in occupied slots (ms)=0 Total time spent by all reduces in occupied slots (ms)=227232 Total time spent by all reduce tasks (ms)=7101 Total vcore-seconds taken by all reduce tasks=7101 Total megabyte-seconds taken by all reduce tasks=7271424 Map-Reduce Framework Combine input records=0 Combine output records=0 Reduce input groups=0 Reduce shuffle bytes=0 Reduce input records=0 Reduce output records=0 Spilled Records=0 Shuffled Maps =0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=67 CPU time spent (ms)=290 Physical memory (bytes) snapshot=94629888 Virtual memory (bytes) snapshot=2064699392 Total committed heap usage (bytes)=30474240 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Output Format Counters Bytes Written=0 [root@realtime-1 ~]# 通过下面的命令停止Active状态节点对应的ResourceManager进程（笔者写作过程中对应的为realtime-1节点，同学们需要根据实际情况来确定） ssh realtime-1 \"yarn-daemon.sh stop resourcemanager\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh realtime-1 \"yarn-daemon.sh stop resourcemanager\" stopping resourcemanager [root@realtime-1 ~]# 通过下面的命令查看对应节点的进程信息： ssh realtime-1 \"jps\" 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# ssh realtime-1 \"jps\" 9427 DataNode 2597 QuorumPeerMain 9625 JournalNode 9818 DFSZKFailoverController 10140 NodeManager 11885 Jps 1743 VmServer.jar [root@realtime-1 ~]# 由返回结果可以看出，ResourceManager进程已经被停止\n通过下面的命令，再次使用Hadoop自带的案例测试MapReduce应用程序的运行： hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output1 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output1 18/11/30 16:50:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/11/30 16:50:31 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/11/30 16:50:32 INFO input.FileInputFormat: Total input paths to process : 0 18/11/30 16:50:32 INFO mapreduce.JobSubmitter: number of splits:0 18/11/30 16:50:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1543567750404_0001 18/11/30 16:50:33 INFO impl.YarnClientImpl: Submitted application application_1543567750404_0001 18/11/30 16:50:33 INFO mapreduce.Job: The url to track the job: http://realtime-2:8089/proxy/application_1543567750404_0001/ 18/11/30 16:50:33 INFO mapreduce.Job: Running job: job_1543567750404_0001 18/11/30 16:50:45 INFO mapreduce.Job: Job job_1543567750404_0001 running in uber mode : false 18/11/30 16:50:45 INFO mapreduce.Job: map 0% reduce 0% 18/11/30 16:50:53 INFO mapreduce.Job: map 0% reduce 100% 18/11/30 16:50:54 INFO mapreduce.Job: Job job_1543567750404_0001 completed successfully 18/11/30 16:50:54 INFO mapreduce.Job: Counters: 38 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=119358 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=0 HDFS: Number of bytes written=0 HDFS: Number of read operations=3 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched reduce tasks=1 Total time spent by all maps in occupied slots (ms)=0 Total time spent by all reduces in occupied slots (ms)=147936 Total time spent by all reduce tasks (ms)=4623 Total vcore-seconds taken by all reduce tasks=4623 Total megabyte-seconds taken by all reduce tasks=4733952 Map-Reduce Framework Combine input records=0 Combine output records=0 Reduce input groups=0 Reduce shuffle bytes=0 Reduce input records=0 Reduce output records=0 Spilled Records=0 Shuffled Maps =0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=81 CPU time spent (ms)=280 Physical memory (bytes) snapshot=94146560 Virtual memory (bytes) snapshot=2064695296 Total committed heap usage (bytes)=30474240 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Output Format Counters Bytes Written=0 [root@realtime-1 ~]# 由返回结果可以看出，此时YARN依然可以可靠的实现任务的调度\n在终端模拟器中，通过下面的命令可以查看逻辑ID为rm2（实际映射的节点为realtime-2）的节点对应的ResourceManager状态（同学们需要根据实际情况来确定）： yarn rmadmin -getServiceState rm2 命令运行后的返回结果如下所示：\n[root@realtime-1 ~]# yarn rmadmin -getServiceState rm2 18/11/30 16:51:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable active [root@realtime-1 ~]# 由返回结果可以看出，当前节点已经自动成功接替变成了active状态\n","wordCount":"19581","inLanguage":"zh-cn","datePublished":"2022-10-10T16:04:23+08:00","dateModified":"2022-12-10T16:04:23+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://waite.wang/posts/bigdata/hadoop-install-and-config/"},"publisher":{"@type":"Organization","name":"隶笔难书","logo":{"@type":"ImageObject","url":"https://waite.wang/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://waite.wang/ accesskey=h title="隶笔难书 (Alt + H)">隶笔难书</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://waite.wang/ title=首页><span>首页</span></a></li><li><a href=https://waite.wang/search title=搜索><span>搜索</span></a></li><li><a href=https://waite.wang/categories title=分类><span>分类</span></a></li><li><a href=https://waite.wang/tags title=标签><span>标签</span></a></li><li><a href=https://waite.wang/archives title=归档><span>归档</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>const elementCode=".mermaid",loadMermaid=function(e){mermaid.initialize({theme:e}),mermaid.init({theme:e},document.querySelectorAll(elementCode))},saveOriginalData=function(){return new Promise((e,t)=>{try{var n=document.querySelectorAll(elementCode),s=n.length;n.forEach(t=>{t.setAttribute("data-original-code",t.innerHTML),s--,s==0&&e()})}catch(e){t(e)}})},resetProcessed=function(){return new Promise((e,t)=>{try{var n=document.querySelectorAll(elementCode),s=n.length;n.forEach(t=>{t.getAttribute("data-original-code")!=null&&(t.removeAttribute("data-processed"),t.innerHTML=t.getAttribute("data-original-code")),s--,s==0&&e()})}catch(e){t(e)}})};saveOriginalData().catch(console.error);let isdark=document.body.className.includes("dark");isdark?resetProcessed().then(loadMermaid("dark")).catch(console.error):resetProcessed().then(loadMermaid("neutral")).catch(console.error),document.getElementById("theme-toggle").addEventListener("click",()=>{resetProcessed(),document.body.className.includes("dark")?loadMermaid("neutral"):loadMermaid("dark").catch(console.error)})</script><h1 class="post-title entry-hint-parent">hadoop的安装与配置</h1><div class=post-description>hadoop的安装与配置</div><div class=post-meta>Created: &nbsp;<span title='2022-10-10 16:04:23 +0800 +0800'>2022-10-10</span>&nbsp;·&nbsp;40 min&nbsp;·&nbsp;19581 words&nbsp;|&nbsp;<a href=https://github.com/Waite0603/HugoBlog/edit/main/content/posts/bigData/hadoop-install-and-config.md rel="noopener noreferrer" target=_blank>在 GitHub 编辑</a>
&nbsp; | Last Updated:&nbsp;2022-12-10</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#%e5%ae%89%e8%a3%85jdk aria-label=安装JDK>安装JDK</a></li><li><a href=#%e4%b8%bb%e6%9c%ba%e5%90%8d%e9%85%8d%e7%bd%ae aria-label=主机名配置>主机名配置</a></li><li><a href=#selinux%e5%ae%89%e5%85%a8%e9%85%8d%e7%bd%ae aria-label=SElinux安全配置>SElinux安全配置</a></li><li><a href=#%e9%85%8d%e7%bd%aessh%e5%85%8d%e5%af%86%e7%a0%81%e7%99%bb%e5%bd%95 aria-label=配置SSH免密码登录>配置SSH免密码登录</a></li><li><a href=#%e5%ae%89%e8%a3%85hadoop aria-label=安装Hadoop>安装Hadoop</a></li><li><a href=#hadoop%e8%bf%90%e8%a1%8c%e5%8f%8a%e6%b5%8b%e8%af%95 aria-label=Hadoop运行及测试>Hadoop运行及测试</a></li><li><a href=#1-%e5%ae%9e%e9%aa%8c%e7%9b%ae%e7%9a%84 aria-label="1 、实验目的">1 、实验目的</a></li><li><a href=#2%e5%ae%9e%e9%aa%8c%e5%8e%9f%e7%90%86 aria-label=2、实验原理>2、实验原理</a></li></ul><li><a href=#1-%e9%9b%86%e7%be%a4%e8%8a%82%e7%82%b9%e5%9f%ba%e6%9c%ac%e9%85%8d%e7%bd%ae aria-label="1 集群节点基本配置">1 集群节点基本配置</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-%e8%8a%82%e7%82%b9ip%e5%9c%b0%e5%9d%80%e6%9f%a5%e8%af%a2 aria-label="步骤1. 节点IP地址查询">步骤1. 节点IP地址查询</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-%e8%8a%82%e7%82%b9%e4%b8%bb%e6%9c%ba%e5%90%8d%e9%85%8d%e7%bd%ae aria-label="步骤2. 节点主机名配置">步骤2. 节点主机名配置</a></li><li><a href=#%e6%ad%a5%e9%aa%a43-%e8%8a%82%e7%82%b9123%e4%b8%bb%e6%9c%ba%e5%90%8d%e4%b8%8eip%e5%9c%b0%e5%9d%80%e6%98%a0%e5%b0%84%e6%96%87%e4%bb%b6%e9%85%8d%e7%bd%ae aria-label="步骤3. 节点1、2、3主机名与IP地址映射文件配置">步骤3. 节点1、2、3主机名与IP地址映射文件配置</a></li></ul></li><li><a href=#2-%e9%85%8d%e7%bd%aessh%e5%85%8d%e5%af%86%e7%a0%81%e7%99%bb%e5%bd%95 aria-label="2 配置SSH免密码登录">2 配置SSH免密码登录</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-%e8%8a%82%e7%82%b9123%e7%a7%98%e9%92%a5%e9%85%8d%e7%bd%ae%e5%8f%8a%e5%88%86%e5%8f%91 aria-label="步骤1. 节点1、2、3秘钥配置及分发">步骤1. 节点1、2、3秘钥配置及分发</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-ssh%e5%85%8d%e5%af%86%e7%a0%81%e7%99%bb%e5%bd%95%e6%b5%8b%e8%af%95 aria-label="步骤2. SSH免密码登录测试">步骤2. SSH免密码登录测试</a></li></ul></li><li><a href=#3-%e5%ae%89%e8%a3%85%e9%85%8d%e7%bd%aejdk18 aria-label="3 安装配置JDK1.8">3 安装配置JDK1.8</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-%e5%88%9b%e5%bb%ba%e5%b7%a5%e4%bd%9c%e8%b7%af%e5%be%84 aria-label="步骤1. 创建工作路径">步骤1. 创建工作路径</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-%e8%a7%a3%e5%8e%8b%e5%ae%89%e8%a3%85%e5%8c%85 aria-label="步骤2. 解压安装包">步骤2. 解压安装包</a></li><li><a href=#%e6%ad%a5%e9%aa%a43-%e9%85%8d%e7%bd%ae%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f aria-label="步骤3. 配置环境变量">步骤3. 配置环境变量</a></li><li><a href=#%e6%ad%a5%e9%aa%a44-%e6%9b%b4%e6%96%b0%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f aria-label="步骤4. 更新环境变量">步骤4. 更新环境变量</a></li><li><a href=#%e6%ad%a5%e9%aa%a45-%e9%aa%8c%e8%af%81jdk%e6%98%af%e5%90%a6%e9%85%8d%e7%bd%ae%e6%88%90%e5%8a%9f aria-label="步骤5. 验证JDK是否配置成功">步骤5. 验证JDK是否配置成功</a></li></ul></li><li><a href=#4-ntp%e6%9c%8d%e5%8a%a1%e9%85%8d%e7%bd%ae aria-label="4 NTP服务配置">4 NTP服务配置</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-ntp%e6%9c%8d%e5%8a%a1%e9%85%8d%e7%bd%ae aria-label="步骤1. NTP服务配置">步骤1. NTP服务配置</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-%e5%90%af%e5%8a%a8ntp%e6%9c%8d%e5%8a%a1 aria-label="步骤2. 启动NTP服务">步骤2. 启动NTP服务</a></li><li><a href=#%e6%ad%a5%e9%aa%a43-ntp%e6%9c%8d%e5%8a%a1%e7%8a%b6%e6%80%81%e6%9f%a5%e7%9c%8b aria-label="步骤3. NTP服务状态查看">步骤3. NTP服务状态查看</a></li></ul></li><li><a href=#5-selinux%e5%ae%89%e5%85%a8%e9%85%8d%e7%bd%ae aria-label="5 SElinux安全配置">5 SElinux安全配置</a></li><li><a href=#6-%e5%ae%89%e8%a3%85%e9%85%8d%e7%bd%aezookeeper%e9%9b%86%e7%be%a4 aria-label="6 安装配置ZooKeeper集群">6 安装配置ZooKeeper集群</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-%e8%a7%a3%e5%8e%8b%e5%ae%89%e8%a3%85%e5%8c%85 aria-label="步骤1. 解压安装包">步骤1. 解压安装包</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e5%88%9b%e5%bb%ba aria-label="步骤2. 数据存储目录创建">步骤2. 数据存储目录创建</a></li><li><a href=#%e6%ad%a5%e9%aa%a43-%e4%b8%bb%e6%9c%bamyid%e7%bc%96%e5%8f%b7%e6%96%87%e4%bb%b6%e5%88%9b%e5%bb%ba aria-label="步骤3. 主机myid编号文件创建">步骤3. 主机myid编号文件创建</a></li><li><a href=#%e6%ad%a5%e9%aa%a44-zookeeper%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e7%bc%96%e8%be%91 aria-label="步骤4. ZooKeeper配置文件编辑">步骤4. ZooKeeper配置文件编辑</a></li><li><a href=#%e6%ad%a5%e9%aa%a45-%e6%96%87%e4%bb%b6%e5%88%86%e5%8f%91 aria-label="步骤5. 文件分发">步骤5. 文件分发</a></li><li><a href=#%e6%ad%a5%e9%aa%a46-zookeeper%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e9%85%8d%e7%bd%ae aria-label="步骤6. ZooKeeper环境变量配置">步骤6. ZooKeeper环境变量配置</a></li><li><a href=#%e6%ad%a5%e9%aa%a47-%e6%9b%b4%e6%96%b0%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f aria-label="步骤7. 更新环境变量">步骤7. 更新环境变量</a></li><li><a href=#%e6%ad%a5%e9%aa%a48-%e9%aa%8c%e8%af%81%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e6%98%af%e5%90%a6%e9%85%8d%e7%bd%ae%e6%88%90%e5%8a%9f aria-label="步骤8. 验证环境变量是否配置成功">步骤8. 验证环境变量是否配置成功</a></li></ul></li><li><a href=#7-zookeeper%e5%90%af%e5%8a%a8%e5%8f%8a%e7%8a%b6%e6%80%81%e6%9f%a5%e7%9c%8b aria-label="7 ZooKeeper启动及状态查看">7 ZooKeeper启动及状态查看</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-zookeeper%e5%90%af%e5%8a%a8 aria-label="步骤1. ZooKeeper启动">步骤1. ZooKeeper启动</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-zookeeper%e8%bf%90%e8%a1%8c%e7%8a%b6%e6%80%81%e6%9f%a5%e7%9c%8b aria-label="步骤2. ZooKeeper运行状态查看">步骤2. ZooKeeper运行状态查看</a></li></ul></li><li><a href=#8-%e5%ae%89%e8%a3%85%e9%85%8d%e7%bd%aehadoop%e9%9b%86%e7%be%a4 aria-label="8 安装配置Hadoop集群">8 安装配置Hadoop集群</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8%e7%9b%ae%e5%bd%95%e5%88%9b%e5%bb%ba aria-label="步骤1. 数据存储目录创建">步骤1. 数据存储目录创建</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-%e8%a7%a3%e5%8e%8b%e5%ae%89%e8%a3%85%e6%96%87%e4%bb%b6 aria-label="步骤2. 解压安装文件">步骤2. 解压安装文件</a></li><li><a href=#%e6%ad%a5%e9%aa%a43-%e7%bc%96%e8%be%91hadoop%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6 aria-label="步骤3. 编辑Hadoop配置文件：">步骤3. 编辑Hadoop配置文件：</a></li><li><a href=#%e6%ad%a5%e9%aa%a44-%e6%96%87%e4%bb%b6%e5%88%86%e5%8f%91 aria-label="步骤4. 文件分发">步骤4. 文件分发</a></li><li><a href=#%e6%ad%a5%e9%aa%a45-%e9%85%8d%e7%bd%aehadoop%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f aria-label="步骤5. 配置Hadoop环境变量">步骤5. 配置Hadoop环境变量</a></li><li><a href=#%e6%ad%a5%e9%aa%a46-%e6%9b%b4%e6%96%b0%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f aria-label="步骤6. 更新环境变量">步骤6. 更新环境变量</a></li><li><a href=#%e6%ad%a5%e9%aa%a47-%e9%aa%8c%e8%af%81hadoop%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e6%98%af%e5%90%a6%e9%85%8d%e7%bd%ae%e6%88%90%e5%8a%9f aria-label="步骤7. 验证Hadoop环境变量是否配置成功">步骤7. 验证Hadoop环境变量是否配置成功</a></li><li><a href=#%e6%ad%a5%e9%aa%a48-%e6%a0%bc%e5%bc%8f%e5%8c%96hdfs aria-label="步骤8. 格式化HDFS">步骤8. 格式化HDFS</a></li><li><a href=#%e6%ad%a5%e9%aa%a49-%e6%a0%bc%e5%bc%8f%e5%8c%96zkfc%e5%85%83%e6%95%b0%e6%8d%ae aria-label="步骤9. 格式化zkfc元数据">步骤9. 格式化zkfc元数据</a></li></ul></li><li><a href=#9-hadoop%e9%9b%86%e7%be%a4%e5%90%af%e5%8a%a8%e8%bf%90%e8%a1%8c aria-label="9 Hadoop集群启动运行">9 Hadoop集群启动运行</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-%e5%90%af%e5%8a%a8hdfs%e7%9b%b8%e5%85%b3%e6%9c%8d%e5%8a%a1 aria-label="步骤1. 启动HDFS相关服务">步骤1. 启动HDFS相关服务</a></li></ul></li><li><a href=#10-hadoop-%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7%e6%b5%8b%e8%af%95 aria-label="10 Hadoop 高可用性测试">10 Hadoop 高可用性测试</a><ul><li><a href=#%e6%ad%a5%e9%aa%a41-nodemanager%e7%8a%b6%e6%80%81%e6%9f%a5%e7%9c%8b aria-label="步骤1. NodeManager状态查看">步骤1. NodeManager状态查看</a></li><li><a href=#%e6%ad%a5%e9%aa%a42-resourcemanager%e7%8a%b6%e6%80%81%e6%9f%a5%e7%9c%8b aria-label="步骤2. ResourceManager状态查看">步骤2. ResourceManager状态查看</a></li><li><a href=#%e6%ad%a5%e9%aa%a43-hdfs%e9%ab%98%e5%8f%af%e7%94%a8%e6%b5%8b%e8%af%95 aria-label="步骤3. HDFS高可用测试">步骤3. HDFS高可用测试</a></li><li><a href=#%e6%ad%a5%e9%aa%a44-yarn%e9%ab%98%e5%8f%af%e7%94%a8%e6%b5%8b%e8%af%95 aria-label="步骤4. YARN高可用测试">步骤4. YARN高可用测试</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h3 id=安装jdk>安装JDK<a hidden class=anchor aria-hidden=true href=#安装jdk>#</a></h3><p><strong>1.创建工作路径</strong></p><pre class=mermaid>mkdir /usr/cx
</pre><p><strong>2.解压安装包</strong></p><pre class=mermaid>tar -zxvf 安装包位置 -C /usr/cx
</pre><p><strong>3.配置环境变量</strong></p><pre class=mermaid>vi ~/.bashrc
</pre><p>在打开的~/.bashrc文件中写入一下内容</p><pre class=mermaid># .bashrc

# User specific aliases and functions

alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
(----------------注：需要在此处增加内容-------------------)
--在这添加--
export JAVA_HOME=/usr/cx/jdk名字版本
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar
</pre><p><strong>4.更新环境变量</strong></p><pre class=mermaid>source ~/.bashrc
</pre><p><strong>5.验证jdk是否配置成功</strong></p><pre class=mermaid>java -version
</pre><h3 id=主机名配置>主机名配置<a hidden class=anchor aria-hidden=true href=#主机名配置>#</a></h3><p><strong>1.编辑主机名</strong></p><pre class=mermaid>vi /etc/sysconfig/network
</pre><p>打开后的文件如下</p><pre class=mermaid>NETWORKING=yes
HOSTNAME=CentOS6.5     -----将此地方更改为localhost  ----注意若为本地主机则更改为localhost不是则更改为别的地址
</pre><p>更给后输入reboot重启</p><pre class=mermaid>reboot
</pre><p><strong>2.IP地址与主机名映射文件配置</strong></p><pre class=mermaid>vi /etc/hosts
</pre><p>打开后的文件如下</p><pre class=mermaid>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 (注：在此行增加内容)

::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

在第一行的ip地址后面添加一个localhost  ----注意若为本地主机则更改为localhost不是则更改为别的地址
</pre><p><strong>3.检测主机名与IP映射是否配置成功</strong></p><pre class=mermaid>ping localhost -c 4
</pre><h3 id=selinux安全配置>SElinux安全配置<a hidden class=anchor aria-hidden=true href=#selinux安全配置>#</a></h3><p><strong>1.关闭SElinux</strong></p><p>通过命令使用vi编辑器打开SElinux配置文件</p><pre class=mermaid>vi /etc/selinux/config
</pre><p>打开后的文件如下</p><pre class=mermaid># This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=permissive  ------ 将这里的值更改为disabled      (注：需要更改此行内容)
# SELINUXTYPE= can take one of these two values:
#     targeted - Targeted processes are protected,
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
</pre><p><strong>2.SElinux配置强制生效</strong></p><pre class=mermaid>setenforce 0
</pre><h3 id=配置ssh免密码登录>配置SSH免密码登录<a hidden class=anchor aria-hidden=true href=#配置ssh免密码登录>#</a></h3><p><strong>1.生成密钥</strong></p><p>输入一下命令生成本机密钥文件</p><pre class=mermaid>ssh-keygen -t dsa
</pre><p>当出现提示的时候，我们直接按回车即可，默认会将秘钥文件生成到~/.ssh/目录下（由于我们实验所使用的登录用户为root，因此~/.ssh/等同于/root/.ssh/）</p><p>通过一下命令查看~/.ssh目录下的文件</p><pre class=mermaid>ls ~/.ssh
</pre><p><strong>2.密钥分发</strong></p><p>把当前节点的公钥文件id_dsa.pub内容输出追加到任意节点的~/.ssh/authorized_keys文件的末尾，则在被添加的节点上便可以免密码登录到当前的节点（由于我们是单节点部署，因此直接追加到当前节点的~/.ssh/authorized_keys文件中即可</p><pre class=mermaid>cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</pre><p><strong>3.验证免密码登录是否配置成功</strong></p><pre class=mermaid>ssh localhost  ----注意若为本地主机则更改为localhost不是则更改为别的地址
</pre><p>第一次登录的时候，会询问呢是否继续连接，输入yes即可</p><p>连接成功后退出连接</p><pre class=mermaid>exit
</pre><h3 id=安装hadoop>安装Hadoop<a hidden class=anchor aria-hidden=true href=#安装hadoop>#</a></h3><p><strong>1.解压安装文件</strong></p><pre class=mermaid>tar -zxvf Hadoop安装包位置 -C /usr/cx
</pre><p><strong>2.配置Hadoop环境变量</strong></p><pre class=mermaid>vi ~/.bashrc
</pre><p>打开后的文件如下</p><pre class=mermaid># .bashrc

# User specific aliases and functions

alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
export JAVA_HOME=/usr/cx/jdk+版本
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar

(----------------在此处增加内容-------------------)

export HADOOP_HOME=/usr/cx/hadoop+版本
export PATH=$PATH:$HADOOP_HOME/bin:$PATH
export PATH=$PATH:$HADOOP_HOME/sbin:$PATH
</pre><p>退出后执行如下命令，更新环境变量</p><pre class=mermaid>source ~/.bashrc
</pre><p>通过下列命令验证Hadoop环境变量是否配置成功</p><pre class=mermaid>hadoop
</pre><p><strong>3.编辑Hadoop配置文件</strong></p><p>使用vi命令打开hadoop-env.sh配置文件进行编辑:</p><pre class=mermaid>vi /usr/cx/hadoop版本/etc/hadoop/hadoop-env.sh
</pre><p>打开后的文件如下</p><pre class=mermaid># Set Hadoop-specific environment variables here.

# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
export JAVA_HOME=${JAVA_HOME}  ---更改为 export  JAVA_HOME=/usr/cx/jdk+版本      (注：需要对此行内容进行更改，为Hadoop绑定Java运行环境)

# The jsvc implementation to use. Jsvc is required to run secure datanodes
# that bind to privileged ports to provide authentication of data transfer
# protocol.  Jsvc is not required if SASL is configured for authentication of
# data transfer protocol using non-privileged ports.
#export JSVC_HOME=${JSVC_HOME}

export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-&#34;/etc/hadoop&#34;}
</pre><p>使用vi命令打开core-site.xml配置文件进行编辑</p><pre class=mermaid>vi /usr/cx/hadoop+版本/etc/hadoop/core-site.xml
</pre><p>打开的文件内容如下</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;

&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
/*设置默认的HDFS访问路径*/
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/*缓冲区大小：io.file.buffer.size默认是4KB*/
&lt;property&gt;
&lt;name&gt;io.file.buffer.size&lt;/name&gt;
&lt;value&gt;131072&lt;/value&gt;
&lt;/property&gt;
/*临时文件夹路径设置*/
&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;file:/usr/tmp&lt;/value&gt;
&lt;/property&gt;
/*设置使用hduser用户可以代理所有主机用户进行任务提交*/
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
/*设置使用hduser用户可以代理所有组用户进行任务提交*/
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre><p>退出vi编辑器后输入以下vi命令打开yarn-site.xml文件进行配置</p><pre class=mermaid>vi /usr/cx/hadoop+版本/etc/hadoop/yarn-site.xml
</pre><p>打开后的文件内容如下</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;
&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
&lt;!-- Site specific YARN configuration properties --&gt;
/*设置NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
/*设置客户端与ResourceManager的通信地址*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
&lt;value&gt;localhost:8032&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/*设置ApplicationMaster调度器与ResourceManager的通信地址*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
&lt;value&gt;localhost:8030&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/*设置NodeManager与ResourceManager的通信地址*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
&lt;value&gt;localhost:8031&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/*设置管理员与ResourceManager的通信地址*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
&lt;value&gt;localhost:8033&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/* ResourceManager的Web地址，监控资源调度*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
&lt;value&gt;localhost:8088&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
&lt;/configuration&gt;
</pre><p>使用下列命令复制mapred-site.xml.template文件并重命名为mapred-site.xml：</p><pre class=mermaid>cp /usr/cx/hadoop+版本/etc/hadoop/mapred-site.xml.template /usr/cx/hadoop+版本/etc/hadoop/mapred-site.xml
</pre><p>使用vi命令打开mapred-site.xml文件进行配置：</p><pre class=mermaid>vi /usr/cx/hadoop+版本/etc/hadoop/mapred-site.xml
</pre><p>打开后的文件内容如下</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34;?&gt;
&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;

&lt;!-- Put site-specific property overrides in this file. --&gt;

&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
/*Hadoop对MapReduce运行框架一共提供了3种实现，在mapred-site.xml中通过&#34;mapreduce.framework.name&#34;这个属性来设置为&#34;classic&#34;、&#34;yarn&#34;或者&#34;local&#34;*/
&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
/*MapReduce JobHistory Server地址*/
&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
&lt;value&gt;localhost:10020&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/*MapReduce JobHistory Server Web UI访问地址*/
&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
&lt;value&gt;localhost:19888&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;

&lt;/configuration&gt;
</pre><p>执行以下命令创建Hadoop的数据存储目录namenode和datanode</p><pre class=mermaid>mkdir -p /hdfs/namenode
mkdir -p /hdfs/datanode
</pre><p>使用vi命令打开hdfs-site.xml文件进行配置：</p><pre class=mermaid>vi /usr/cx/hadoop+版本/etc/hadoop/hdfs-site.xml
</pre><p>打开的文件内容如下</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;

&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
/*配置SecondaryNameNode地址*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
&lt;value&gt;localhost:9001&lt;/value&gt;  ----注意若为本地主机则更改为localhost不是则更改为别的地址
&lt;/property&gt;
/*配置NameNode的数据存储目录，需要与上文创建的目录相对应*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;file:/hdfs/namenode&lt;/value&gt;
&lt;/property&gt;
/*配置DataNode的数据存储目录，需要与上文创建的目录相对应*/
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;file:/hdfs/datanode&lt;/value&gt;
&lt;/property&gt;
/*配置数据块副本数*/
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
/*将dfs.webhdfs.enabled属性设置为true，否则就不能使用webhdfs的LISTSTATUS、LIST FILESTATUS等需要列出文件、文件夹状态的命令，因为这些信息都是由namenode保存的*/
&lt;property&gt;
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre><p>使用vi命令打开slaves文件进行配置（要与我们前文设置的主机名相互一致，否则将会引起Hadoop相关进程无法正确启动）：</p><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/slaves
</pre><p>打开的文件内容如下</p><pre class=mermaid>localhost  ----注意若为本地主机则更改为localhost不是则更改为别的地址
</pre><p>若为localhost则不做更改，因为在本地机器而不是在易优云中需要连接到易优云的主机</p><p>** 4.格式化HDFS **</p><p>通过下列命令格式化HDFS文件系统</p><pre class=mermaid>hadoop namenode -format
</pre><h3 id=hadoop运行及测试>Hadoop运行及测试<a hidden class=anchor aria-hidden=true href=#hadoop运行及测试>#</a></h3><p>** 1.启动Hadoop**</p><p>通过下列命令启动Hadoop：</p><pre class=mermaid>start-all.sh
</pre><p>通过下列命令，查看相应的JVM进程确定Hadoop是否配置及启动成功：</p><pre class=mermaid>jps
</pre><p>** Web页面测试**</p><p>用浏览器输入网址
<a href=https:// target=_blank>http://localhost:8080</a>和
<a href=https:// target=_blank>http://localhost50070</a></p><p><a href=http://49.234.55.187:8090/archives/hadoop%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE target=_blank></a></p><h3 id=1-实验目的>1 、实验目的<a hidden class=anchor aria-hidden=true href=#1-实验目的>#</a></h3><p>通过本节实验的学习，同学们可以掌握Hadoop集群环境部署与配置。本实验完成后，要求学生掌握以下内容：</p><ol><li><p>掌握集群所有节点之间SSH免密登录配置方式；</p></li><li><p>掌握NTP服务配置，实现节点间的时间同步；</p></li><li><p>掌握ZooKeeper集群的搭建方式；</p></li><li><p>掌握Hadoop集群的搭建配置流程；</p></li><li><p>理解Hadoop集群的高可用（HA）原理，并掌握Hadoop集群的高可用（HA）配置方法。</p></li></ol><h3 id=2实验原理>2、实验原理<a hidden class=anchor aria-hidden=true href=#2实验原理>#</a></h3><p>需要按照以下流程，在Linux上进行Hadoop集群的安装部署：</p><ol><li><p>主机名配置：在大型的Hadoop集群中，往往由成百上千个节点组成，如果通过IP地址对不同节点进行管理，那么集群维护的工作量将会十分繁重，因此在工程环境中，常常通过对每个节点设置唯一的主机名，从而实现对节点进行管理。</p></li><li><p>SSH（安全外壳协议）免密码登录配置：推荐安装OpenSSH。Hadoop需要通过SSH来启动Slave列表中各台主机的守护进程，因此SSH也是必须安装的。</p></li><li><p>安装配置JDK1.7（或更高版本）：Hadoop是用Java编写的程序，Hadoop的编译及MapReduce的运行都需要使用JDK，因此在安装Hadoop前，必须安装JDK1.7或更高版本。</p></li><li><p>NTP服务配置：本实验需要在实现Hadoop集群搭建的同时，并进行高可用性（HA）的配置，因此需要通过ZooKeeper来对集群中的节点进行协调，而ZooKeeper需要保证节点间的时钟相互一致，因此需要在集群中配置NTP服务。</p></li><li><p>SElinux安全配置：CentOS默认启用了SElinux，在网络服务方面权限要求比较严格，因此需要对SElinux安全配置进行更改。</p></li><li><p>ZooKeeper集群搭建：高可用性（HA）Hadoop集群的搭建需要依赖于ZooKeeper来对集群中的节点进行协调，因此需要进行ZooKeeper集群搭建。</p></li><li><p>Hadoop核心配置。Hadoop的稳定运行需要依赖于其核心配置文件，因此当上述准备工作就绪后，我们便需要着重进行配置文件编写来实现Hadoop的可靠运行。</p></li></ol><p>我们需要在<code>节点1</code>、<code>节点2</code>、<code>节点3</code>中进行高可用Hadoop集群环境的部署。各个节点所部署的服务如下所示：</p><table><thead><tr><th>节点1</th><th>节点2</th><th>节点3</th></tr></thead><tbody><tr><td>NameNode</td><td>StandBy</td><td></td></tr><tr><td>ResourceManager</td><td>StandBy</td><td></td></tr><tr><td>DFSZKFailoverController</td><td>DFSZKFailoverController</td><td></td></tr><tr><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>JournalNode</td><td>JournalNode</td><td>JournalNode</td></tr></tbody></table><h2 id=1-集群节点基本配置>1 集群节点基本配置<a hidden class=anchor aria-hidden=true href=#1-集群节点基本配置>#</a></h2><h3 id=步骤1-节点ip地址查询>步骤1. 节点IP地址查询<a hidden class=anchor aria-hidden=true href=#步骤1-节点ip地址查询>#</a></h3><ol><li>在<code>节点1、2、3</code>中通过下面的命令查询节点IP地址：</li></ol><pre class=mermaid>ifconfig
</pre><p>命令运行后的返回结果如下所示 (每台虚拟机的IP地址都是不同的，因此需要以实际地址信息为准）：</p><pre class=mermaid>[root@CentOS6 ~]# ifconfig
eth6      Link encap:Ethernet  HWaddr 02:00:1E:79:09:04 
          inet addr:10.1.1.4  Bcast:10.1.1.255  Mask:255.255.255.0
          inet6 addr: fe80::1eff:fe79:904/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:20832 errors:0 dropped:0 overruns:0 frame:0
          TX packets:13052 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:31392026 (29.9 MiB)  TX bytes:929956 (908.1 KiB)

lo        Link encap:Local Loopback 
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:720 (720.0 b)  TX bytes:720 (720.0 b) 

[root@CentOS6 ~]#
</pre><p>需要记录三个节点的IP地址，在后文中我们需要根据此IP地址进行相关操作</p><h3 id=步骤2-节点主机名配置>步骤2. 节点主机名配置<a hidden class=anchor aria-hidden=true href=#步骤2-节点主机名配置>#</a></h3><p>需要在<code>节点1、2、3</code>进行下列操作，将三个主机名分别配置为realtime-1，realtime-2，realtime-3</p><ol><li>通过下列命令使用vi编辑器编辑主机名配置文件：</li></ol><pre class=mermaid>vi /etc/sysconfig/network
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>NETWORKING=yes

HOSTNAME=CentOS6.5                            (注：需要将此行内容修改为实际的主机名realtime-1、realtime-2、realtime-3)
</pre><ol start=2><li>在文件中进行内容更改，将HOSTNAME字段内容配置成realtime-：</li></ol><pre class=mermaid>HOSTNAME=realtime-1
</pre><p>编辑完成后保存文件并退出vi编辑器</p><p>更改后的文件内容如下所示：</p><ol start=3><li>更改后的内容会在下次系统重启的时候生效，通过下列命令重新启动系统：</li></ol><pre class=mermaid>reboot
</pre><h3 id=步骤3-节点123主机名与ip地址映射文件配置>步骤3. 节点1、2、3主机名与IP地址映射文件配置<a hidden class=anchor aria-hidden=true href=#步骤3-节点123主机名与ip地址映射文件配置>#</a></h3><ol><li>在<code>节点1、2、3</code>中，通过下列命令使用vi编辑器编辑hosts文件：</li></ol><pre class=mermaid>vi /etc/hosts
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 (注：在此行增加内容)
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
</pre><ol start=2><li>增加节点<code>1、2、3</code>的IP地址与主机名的映射关系、节点间的IP地址与主机名的映射关系、节点间的IP地址与主机名的映射关系，IP地址与主机名之间用空格分隔（主机名填写为前文配置的节点实际主机名称，IP地址需要根据上文中的查询结果来进行填写，并与实际的主机名相对应）：</li></ol><pre class=mermaid>10.1.1.4 realtime-1
10.1.1.3 realtime-2
10.1.1.206 realtime-3
</pre><p>更改后的文件内容如下所示</p><pre class=mermaid>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.1.1.4 realtime-1
10.1.1.3 realtime-2
10.1.1.206 realtime-3
</pre><p>编辑完成后保存文件并退出vi编辑器</p><ol start=3><li>通过下列命令检测主机名与IP映射是否配置成功：</li></ol><pre class=mermaid>ping realtime-1 -c 2
</pre><p>如果配置成功，则会显示如下结果：</p><pre class=mermaid>[root@realtime-1 ~]# ping realtime-1 -c 2      (注：通过此命令向realtime-1节点发送2个报文)
PING realtime-1 (10.1.1.4) 56(84) bytes of data.
64 bytes from realtime-1 (10.1.1.4): icmp_seq=1 ttl=64 time=1.98 ms
64 bytes from realtime-1 (10.1.1.4): icmp_seq=2 ttl=64 time=0.341 ms

--- realtime-1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.341/1.163/1.985/0.822 ms
[root@realtime-1 ~]#
</pre><ol start=4><li>通过下列命令检测主机名与IP映射是否配置成功：</li></ol><pre class=mermaid>ping realtime-2 -c 2
</pre><p>如果配置成功，则会显示如下结果：</p><pre class=mermaid>[root@realtime-1 ~]# ping realtime-2 -c 2     (注：通过此命令向realtime-2节点发送2个报文)
PING realtime-2 (10.1.1.3) 56(84) bytes of data.
64 bytes from realtime-2 (10.1.1.3): icmp_seq=1 ttl=64 time=0.047 ms
64 bytes from realtime-2 (10.1.1.3): icmp_seq=2 ttl=64 time=0.026 ms

--- realtime-2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.026/0.036/0.047/0.012 ms
[root@realtime-1 ~]#
</pre><ol start=5><li>通过下列命令检测主机名与IP映射是否配置成功：</li></ol><pre class=mermaid>ping realtime-3 -c 2
</pre><p>如果配置成功，则会显示如下结果：</p><pre class=mermaid>[root@realtime-1 ~]# ping realtime-3 -c 2    (注：通过此命令向realtime-3节点发送2个报文)
PING realtime-3 (10.1.1.206) 56(84) bytes of data.
64 bytes from realtime-3 (10.1.1.206): icmp_seq=1 ttl=64 time=1.36 ms
64 bytes from realtime-3 (10.1.1.206): icmp_seq=2 ttl=64 time=0.315 ms

--- realtime-3 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.315/0.841/1.367/0.526 ms
[root@realtime-1 ~]#
</pre><p><em>如果无法进行正常的报文发送，请检查主机名是否配置正确，同时请检查主机名与IP地址映射是否配置正确。</em></p><h2 id=2-配置ssh免密码登录>2 配置SSH免密码登录<a hidden class=anchor aria-hidden=true href=#2-配置ssh免密码登录>#</a></h2><h3 id=步骤1-节点123秘钥配置及分发>步骤1. 节点1、2、3秘钥配置及分发<a hidden class=anchor aria-hidden=true href=#步骤1-节点123秘钥配置及分发>#</a></h3><p>例如<code>节点1</code> : 需要在节点1进行下列操作，在节点1中生成秘钥文件，然后将公钥文件分发到节点2和节点3中，实现在节点1可以免密码登录到集群中的其他主机中。</p><ol><li>通过下面的命令生成密钥（使用rsa加密方式）：</li></ol><pre class=mermaid>echo -e &#34;\n&#34;|ssh-keygen -t rsa -N &#34;&#34; &gt;/dev/null 2&gt;&amp;1
</pre><p>默认情况下会在~/.ssh/文件夹下生成公钥文件id_rsa.pub和私钥文件id_rsa，通过下面的命令对~/.ssh/内容进行查看：</p><pre class=mermaid>ll ~/.ssh/
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ll ~/.ssh/
总用量 8
-rw-------. 1 root root 1675 11月 29 13:42 id_rsa
-rw-r--r--. 1 root root  397 11月 29 13:42 id_rsa.pub
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下面的命令将公钥文件发送到本机以及其他两个节点，创建root免密钥通道（需要输入密码：111111）：</li></ol><pre class=mermaid>ssh-copy-id -i /root/.ssh/id_rsa.pub root@realtime-1 # 其他的节点需要随之改动root@realtime-2 and root@realtime-3
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@realtime-1
The authenticity of host &#39;realtime-1 (10.1.1.4)&#39; can&#39;t be established.
RSA key fingerprint is 9f:3b:30:10:65:46:c9:c3:2b:fb:e5:28:38:39:9c:84.
Are you sure you want to continue connecting (yes/no)? yes    (注：此处需要输入yes)
Warning: Permanently added &#39;realtime-1,10.1.1.4&#39; (RSA) to the list of known hosts.
root@realtime-1&#39;s password:                      （注：此处需要输入root用户密码，为111111）
Now try logging into the machine, with &#34;ssh &#39;root@realtime-1&#39;&#34;, and check in:

  .ssh/authorized_keys

to make sure we haven&#39;t added extra keys that you weren&#39;t expecting.

[root@realtime-1 ~]#
</pre><h3 id=步骤2-ssh免密码登录测试>步骤2. SSH免密码登录测试<a hidden class=anchor aria-hidden=true href=#步骤2-ssh免密码登录测试>#</a></h3><p>集群中各个节点秘钥分发完毕后，可以通过ssh远程登录命令来测试免密码登录是否配置成功。为了操作统一，我们在节点3中进行下面的操作（在其他节点操作所实现的效果也是一样的）</p><ol><li>在节点3中通过下面的命令可以实现免密码远程登录到节点1：</li></ol><pre class=mermaid>ssh realtime-1 #依次运行realtime-2 and realtime-3
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-3 ~]# ssh realtime-1
Last login: Thu Nov 29 14:08:34 2018 from realtime-3
[root@realtime-1 ~]#
</pre><p>如果从源主机到目的主机的登录过程中，出现需要输入密码的情况，那么需要检查是否已经成功将源主机的公钥文件发送到目的主机中</p><h2 id=3-安装配置jdk18>3 安装配置JDK1.8<a hidden class=anchor aria-hidden=true href=#3-安装配置jdk18>#</a></h2><p>JDK需要在集群3个节点都进行安装，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行</p><p>我们可以在Oracle JDK的官网下载相应版本的JDK，官网地址为:
<a href=https:// target=_blank>http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p><h3 id=步骤1-创建工作路径>步骤1. 创建工作路径<a hidden class=anchor aria-hidden=true href=#步骤1-创建工作路径>#</a></h3><ol><li>首先需要在终端中输入下列命令，在/usr目录下建立cx工作路径：</li></ol><pre class=mermaid>mkdir /usr/cx
</pre><ol start=2><li>通过下面的命令实现在节点2和节点3的/usr目录下建立cx工作路径：</li></ol><pre class=mermaid>ssh realtime-2 &#34;mkdir /usr/cx&#34; 
ssh realtime-3 &#34;mkdir /usr/cx&#34;
</pre><h3 id=步骤2-解压安装包>步骤2. 解压安装包<a hidden class=anchor aria-hidden=true href=#步骤2-解压安装包>#</a></h3><ol><li>我们可以在/usr/software/目录下找到jdk-8u60-linux-x64.tar.gz安装包，通过下列命令将其解压到/usr/cx/目录下：</li></ol><pre class=mermaid>tar -zxvf /usr/software/jdk-8u60-linux-x64.tar.gz -C /usr/cx
</pre><p>命令执行后的输出内容如下所示：</p><pre class=mermaid>(-------------------省略------------------------)
jdk1.8.0_60/bin/jmc.ini
jdk1.8.0_60/bin/jmap
jdk1.8.0_60/bin/serialver
jdk1.8.0_60/bin/wsgen
jdk1.8.0_60/bin/jrunscript
jdk1.8.0_60/bin/javah
jdk1.8.0_60/bin/javac
jdk1.8.0_60/bin/jvisualvm
jdk1.8.0_60/bin/jcontrol
jdk1.8.0_60/release
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下列命令实现在节点2和节点3中将jdk-8u60-linux-x64.tar.gz安装包解压到/usr/cx/目录下：</li></ol><pre class=mermaid>ssh realtime-2 &#34;tar -zxvf /usr/software/jdk-8u60-linux-x64.tar.gz -C /usr/cx&#34;

ssh realtime-3 &#34;tar -zxvf /usr/software/jdk-8u60-linux-x64.tar.gz -C /usr/cx&#34;
</pre><h3 id=步骤3-配置环境变量>步骤3. 配置环境变量<a hidden class=anchor aria-hidden=true href=#步骤3-配置环境变量>#</a></h3><ol><li>通过下列命令使用vi编辑器打开 ~/.bashrc文件：</li></ol><pre class=mermaid>vi ~/.bashrc
</pre><p>打开的~/.bashrc文件内容如下所示：</p><pre class=mermaid># .bashrc

# User specific aliases and functions

alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
(----------------注：需要在此处增加内容-------------------)
</pre><ol start=2><li>在文件中写入下列内容：</li></ol><pre class=mermaid>export JAVA_HOME=/usr/cx/jdk1.8.0_60
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar
</pre><p>编辑完成后保存文件并退出vi编辑器。</p><ol start=3><li>通过下面的命令将环境变量配置文件分发到节点2和节点3：</li></ol><pre class=mermaid>scp ~/.bashrc root@realtime-2:~/.bashrc  
</pre><pre class=mermaid>scp ~/.bashrc root@realtime-3:~/.bashrc
</pre><p>命令执行后的输出内容如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# scp ~/.bashrc root@realtime-2:~/.bashrc
.bashrc                                      100%  320     0.3KB/s   00:00   
[root@realtime-1 ~]#
</pre><h3 id=步骤4-更新环境变量>步骤4. 更新环境变量<a hidden class=anchor aria-hidden=true href=#步骤4-更新环境变量>#</a></h3><ol><li>执行如下命令，更新环境变量：</li></ol><pre class=mermaid>source  ~/.bashrc
</pre><ol start=2><li>执行如下命令，更新节点2和节点3的环境变量：</li></ol><pre class=mermaid>ssh realtime-2 &#34;source  ~/.bashrc&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;source  ~/.bashrc&#34;
</pre><h3 id=步骤5-验证jdk是否配置成功>步骤5. 验证JDK是否配置成功<a hidden class=anchor aria-hidden=true href=#步骤5-验证jdk是否配置成功>#</a></h3><ol><li>通过下面的命令验证JDK是否安装并配置成功：</li></ol><pre class=mermaid>java -version
</pre><p>如果出现如下JDK版本信息，则说明安装配置成功：</p><pre class=mermaid>[root@realtime-1 ~]# java -version
java version &#34;1.8.0_60&#34;                                              (注：JDK版本号)
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)              (注：Java运行环境版本号)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下面的命令验证<code>节点2</code>和<code>节点3</code>的JDK是否安装并配置成功：</li></ol><pre class=mermaid>ssh realtime-2 &#34;java -version&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;java -version&#34;
</pre><p>如果没有正确输出相关版本信息，请检查~/.bashrc文件中的JDK环境变量是否配置正确，同时请确定是否使用source ~/.bashrc命令更新环境变量配置</p><h2 id=4-ntp服务配置>4 NTP服务配置<a hidden class=anchor aria-hidden=true href=#4-ntp服务配置>#</a></h2><p>需要在集群的3台节点中都进行NTP服务的配置</p><h3 id=步骤1-ntp服务配置>步骤1. NTP服务配置<a hidden class=anchor aria-hidden=true href=#步骤1-ntp服务配置>#</a></h3><ol><li>在<code>节点1</code>、<code>节点2</code>、<code>节点3</code>中通过下面的命令打开NTP配置文件：</li></ol><pre class=mermaid>vi /etc/ntp.conf
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>（---------------省略----------------）
# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

# Hosts on local network are less restricted.
#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 0.centos.pool.ntp.org iburst  （注：注释此行内容）
server 1.centos.pool.ntp.org iburst  （注：注释此行内容）
server 2.centos.pool.ntp.org iburst  （注：注释此行内容）
server 3.centos.pool.ntp.org iburst  （注：注释此行内容）
（注：在此处增加内容）
#broadcast 192.168.1.255 autokey        # broadcast server
（---------------省略----------------）
</pre><p>在文件中进行下列内容更改（通过server字段设置本机为NTP Serevr服务器，通过restrict限制realtime-2和realtime-3主机名对应的主机可以同步时间）：</p><pre class=mermaid>#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10
restrict realtime-2 nomodify notrap
restrict realtime-3 nomodify notrap
</pre><p>更改完成后保存文件并退出编辑器</p><h3 id=步骤2-启动ntp服务>步骤2. 启动NTP服务<a hidden class=anchor aria-hidden=true href=#步骤2-启动ntp服务>#</a></h3><p>为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行。</p><ol><li>通过下面的命令在节点1中设定NTP服务自启动：</li></ol><pre class=mermaid>chkconfig ntpd on
</pre><ol start=2><li>通过下面的命令在节点1中启动NTP服务：</li></ol><pre class=mermaid>service ntpd start
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# service ntpd start
正在启动 ntpd：                                            [确定]
[root@realtime-1 ~]#
</pre><ol start=3><li>通过下面的命令在节点2中设定NTP服务自启动：</li></ol><pre class=mermaid>ssh realtime-2 &#34;chkconfig ntpd on&#34;
</pre><ol start=4><li>通过下面的命令在节点2中启动NTP服务：</li></ol><pre class=mermaid>ssh realtime-2 &#34;service ntpd start&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-2 &#34;service ntpd start&#34;
正在启动 ntpd：[确定]
[root@realtime-1 ~]#
</pre><ol start=5><li>通过下面的命令在节点3中设定NTP服务自启动：</li></ol><pre class=mermaid>ssh realtime-3 &#34;chkconfig ntpd on&#34;
</pre><ol start=6><li>通过下面的命令在节点3中启动NTP服务：</li></ol><pre class=mermaid>ssh realtime-3 &#34;service ntpd start&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-3 &#34;service ntpd start&#34;
正在启动 ntpd：[确定]
[root@realtime-1 ~]#
</pre><p><em>如果服务无法正常启动，会出现相关的错误提示信息，只需要根据错误提示进行更改即可。</em></p><h3 id=步骤3-ntp服务状态查看>步骤3. NTP服务状态查看<a hidden class=anchor aria-hidden=true href=#步骤3-ntp服务状态查看>#</a></h3><p>为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行。</p><ol><li>通过下面的命令查看节点1中NTP服务的运行状态：</li></ol><pre class=mermaid>ntpstat
</pre><p>命令运行后的返回结果如下所示（由于节点1是作为Server节点，所以其状态会很快变成synchronised，此时说明服务已经正常启动）：</p><pre class=mermaid>[root@realtime-1 ~]# ntpstat
synchronised to local net at stratum 11
   time correct to within 449 ms
   polling server every 64 s
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下面的命令查看<code>节点2</code>和<code>节点三</code>中NTP服务的运行状态：</li></ol><pre class=mermaid>ssh realtime-2 &#34;ntpstat&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;ntpstat&#34;
</pre><p>命令运行后的返回结果如下所示（由于节点2需要同步节点1的时间，因此需要大概15分钟其状态才会由unsynchronised会变成synchronised，当状态变为synchronised时说明服务已经正常启动）：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-2 &#34;ntpstat&#34;
unsynchronised
   polling server every 64 s
[root@realtime-1 ~]#
</pre><p>服务正常启动后的状态如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-3 &#34;ntpstat&#34;
synchronised to NTP server (10.1.1.4) at stratum 12
   time correct to within 25 ms
   polling server every 64 s
[root@realtime-1 ~]#
</pre><p><em>当3个节点的状态都显示为synchronised时，则表示ntp服务已经启动成功；如果一直显示unsynchronised,可能是配置文件有错误，因此需要检查IP地址是否配置正确。</em></p><p>同学们不必一直等待，可以先进行下文的实验，然后过后再查看NTP服务状态。</p><h2 id=5-selinux安全配置>5 SElinux安全配置<a hidden class=anchor aria-hidden=true href=#5-selinux安全配置>#</a></h2><p>需要在集群3个节点都进行SElinux配置，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行。</p><ol><li>通过下面的命令，关闭<code>节点1</code>、<code>节点2</code>、<code>节点3</code>的SElinux安全设置：</li></ol><pre class=mermaid>/bin/sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config
</pre><pre class=mermaid>ssh realtime-2 &#34;/bin/sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;/bin/sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config&#34;
</pre><h2 id=6-安装配置zookeeper集群>6 安装配置ZooKeeper集群<a hidden class=anchor aria-hidden=true href=#6-安装配置zookeeper集群>#</a></h2><p>由于我们需要搭建一套具备高可用性的Hadoop集群，因此需要通过ZooKeeper来进行集群中服务的协调。ZooKeeper需要在集群3个节点进行安装配置，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行</p><p>在模板中我们已经将ZooKeeper安装文件zookeeper-3.4.6.tar.gz放到了/usr/software目录下，同学们可以直接使用</p><h3 id=步骤1-解压安装包>步骤1. 解压安装包<a hidden class=anchor aria-hidden=true href=#步骤1-解压安装包>#</a></h3><ol><li>通过下列命令将ZooKeeper安装包解压到/usr/cx目录下：</li></ol><pre class=mermaid>tar -zxvf /usr/software/zookeeper-3.4.6.tar.gz -C /usr/cx
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>(---------------------省略--------------------)
zookeeper-3.4.6/recipes/queue/test/org/apache/zookeeper/recipes/queue/DistributedQueueTest.java
zookeeper-3.4.6/recipes/queue/build.xml
zookeeper-3.4.6/zookeeper-3.4.6.jar
zookeeper-3.4.6/lib/
zookeeper-3.4.6/lib/cobertura/
zookeeper-3.4.6/lib/cobertura/README.txt
zookeeper-3.4.6/lib/jline-0.9.94.jar
zookeeper-3.4.6/lib/log4j-1.2.16.LICENSE.txt
zookeeper-3.4.6/lib/slf4j-log4j12-1.6.1.jar
zookeeper-3.4.6/lib/jdiff/
zookeeper-3.4.6/lib/jdiff/zookeeper_3.1.1.xml
zookeeper-3.4.6/lib/jdiff/zookeeper_3.4.6-SNAPSHOT.xml
zookeeper-3.4.6/lib/jdiff/zookeeper_3.4.6.xml
zookeeper-3.4.6/lib/slf4j-api-1.6.1.jar
zookeeper-3.4.6/lib/log4j-1.2.16.jar
zookeeper-3.4.6/lib/netty-3.7.0.Final.jar
zookeeper-3.4.6/lib/jline-0.9.94.LICENSE.txt
[root@realtime-1 ~]#
</pre><ol start=2><li>解压完成后，我们可以查看解压后的文件夹内容：</li></ol><pre class=mermaid>ls /usr/cx/zookeeper-3.4.6/
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ls /usr/cx/zookeeper-3.4.6/
bin          dist-maven       LICENSE.txt           src
build.xml    docs             NOTICE.txt            zookeeper-3.4.6.jar
CHANGES.txt  ivysettings.xml  README_packaging.txt  zookeeper-3.4.6.jar.asc
conf         ivy.xml          README.txt            zookeeper-3.4.6.jar.md5
contrib      lib              recipes               zookeeper-3.4.6.jar.sha1
[root@realtime-1 ~]#
</pre><h3 id=步骤2-数据存储目录创建>步骤2. 数据存储目录创建<a hidden class=anchor aria-hidden=true href=#步骤2-数据存储目录创建>#</a></h3><ol><li>通过下面的命令创建ZooKeeper数据存储目录：</li></ol><pre class=mermaid>mkdir -p /home/data
</pre><p>通过下面的命令创建ZooKeeper日志存储目录：</p><pre class=mermaid>mkdir -p /home/logs
</pre><ol start=2><li>通过下面的命令在<code>节点2</code>、<code>节点3</code>中创建ZooKeeper数据存储目录：</li></ol><pre class=mermaid>ssh realtime-2 &#34;mkdir -p /home/data&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;mkdir -p /home/data&#34;
</pre><p>通过下面的命令在<code>节点2</code>、<code>节点3</code>中创建ZooKeeper日志存储目录：</p><pre class=mermaid>ssh realtime-2 &#34;mkdir -p /home/logs&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;mkdir -p /home/logs&#34;
</pre><h3 id=步骤3-主机myid编号文件创建>步骤3. 主机myid编号文件创建<a hidden class=anchor aria-hidden=true href=#步骤3-主机myid编号文件创建>#</a></h3><ol><li>通过下面的命令创建myid文件，并设置节点1对应的编号为1（集群启动后会通过此编号来进行主机识别）：</li></ol><pre class=mermaid>echo &#34;1&#34; &gt; /home/data/myid
</pre><ol start=2><li>通过下面的命令在节点2中创建myid文件，并设置节点2对应的编号为2（集群启动后会通过此编号来进行主机识别）：</li></ol><pre class=mermaid>ssh realtime-2 &#34;echo &#34;2&#34; &gt; /home/data/myid&#34;
</pre><ol start=3><li>通过下面的命令在节点3中创建myid文件，并设置节点3对应的编号为3（集群启动后会通过此编号来进行主机识别）：</li></ol><pre class=mermaid>ssh realtime-3 &#34;echo &#34;3&#34; &gt; /home/data/myid&#34;
</pre><h3 id=步骤4-zookeeper配置文件编辑>步骤4. ZooKeeper配置文件编辑<a hidden class=anchor aria-hidden=true href=#步骤4-zookeeper配置文件编辑>#</a></h3><ol><li>通过下列命令创建并打开zoo.cfg配置文件：</li></ol><pre class=mermaid>vi /usr/cx/zookeeper-3.4.6/conf/zoo.cfg
</pre><p>在文件中写入下列内容：</p><pre class=mermaid>tickTime=2000
dataDir=/home/data
clientPort=2181
dataLogDir=/home/logs
initLimit=5
syncLimit=2
server.1=realtime-1:2888:3888
server.2=realtime-2:2888:3888
server.3=realtime-3:2888:3888
</pre><p>编辑完成后保存文件并退出vi编辑器。</p><p>在上述配置中，我们设置心跳时间为2000毫秒，设置ZooKeeper在本地保存数据的目录为/home/data，ZooKeeper监听客户端连接的端口为2181,设置所有Follower和Leader进行同步的时间为5s，设置一个Follower和Leader进行同步的时间为2s。同时设定集群中有3台主机，其中realtime-1对应的主机编号为1，Follower与Leader之间交换信息的端口为2888，进行Leader选举的端口为3888；realtime-2对应的主机编号为2，Follower与Leader之间交换信息的端口为2888，进行Leader选举的端口为3888；realtime-3对应的主机编号为3，Follower与Leader之间交换信息的端口为2888，进行Leader选举的端口为3888。</p><h3 id=步骤5-文件分发>步骤5. 文件分发<a hidden class=anchor aria-hidden=true href=#步骤5-文件分发>#</a></h3><ol><li>通过下面的命令将节点1的ZooKeeper文件包分发到<code>节点2</code>、<code>节点3</code>中：</li></ol><pre class=mermaid>scp -r /usr/cx/zookeeper-3.4.6 root@realtime-2:/usr/cx/
</pre><pre class=mermaid>scp -r /usr/cx/zookeeper-3.4.6 root@realtime-3:/usr/cx/
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>（----------------------省略------------------------）
Makefile.am                                   100%   74     0.1KB/s   00:00   
zkServer.cmd                                  100% 1084     1.1KB/s   00:00   
zkEnv.sh                                      100% 2696     2.6KB/s   00:00   
zkCleanup.sh                                  100% 1937     1.9KB/s   00:00   
zkCli.sh                                      100% 1534     1.5KB/s   00:00   
zkEnv.cmd                                     100% 1333     1.3KB/s   00:00   
zkCli.cmd                                     100% 1049     1.0KB/s   00:00   
README.txt                                    100%  238     0.2KB/s   00:00   
zkServer.sh                                   100% 5742     5.6KB/s   00:00   
NOTICE.txt                                    100%  170     0.2KB/s   00:00   
zookeeper-3.4.6.jar.md5                       100%   33     0.0KB/s   00:00   
README.txt                                    100% 1585     1.6KB/s   00:00   
CHANGES.txt                                   100%   79KB  78.9KB/s   00:00   
zookeeper-3.4.6.jar.sha1                      100%   41     0.0KB/s   00:00   
[root@realtime-1 ~]#
</pre><h3 id=步骤6-zookeeper环境变量配置>步骤6. ZooKeeper环境变量配置<a hidden class=anchor aria-hidden=true href=#步骤6-zookeeper环境变量配置>#</a></h3><ol><li>通过下列命令使用vi编辑器打开 ~/.bashrc文件：</li></ol><pre class=mermaid>vi ~/.bashrc
</pre><p>打开的~/.bashrc文件内容如下所示：</p><pre class=mermaid># .bashrc

# User specific aliases and functions

alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
export JAVA_HOME=/usr/cx/jdk1.8.0_60
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar
(----------------注：需要在此处增加内容-------------------)
</pre><ol start=2><li>在文件中写入下列内容：</li></ol><pre class=mermaid>export ZK_HOME=/usr/cx/zookeeper-3.4.6
export PATH=$PATH:$ZK_HOME/bin
</pre><p>编辑完成后保存文件并退出vi编辑器。</p><ol start=3><li>通过下面的命令将环境变量配置文件分发到<code>节点2</code>和<code>节点3</code>：</li></ol><pre class=mermaid>scp ~/.bashrc root@realtime-2:~/.bashrc
</pre><pre class=mermaid>scp ~/.bashrc root@realtime-3:~/.bashrc
</pre><h3 id=步骤7-更新环境变量>步骤7. 更新环境变量<a hidden class=anchor aria-hidden=true href=#步骤7-更新环境变量>#</a></h3><ol><li>执行如下命令，更新环境变量：</li></ol><pre class=mermaid>source  ~/.bashrc
</pre><pre class=mermaid>ssh realtime-2 &#34;source  ~/.bashrc&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;source  ~/.bashrc&#34;
</pre><h3 id=步骤8-验证环境变量是否配置成功>步骤8. 验证环境变量是否配置成功<a hidden class=anchor aria-hidden=true href=#步骤8-验证环境变量是否配置成功>#</a></h3><ol><li>通过下面的命令验证环境变量是否配置成功：</li></ol><pre class=mermaid>zkServer.sh
</pre><pre class=mermaid>ssh realtime-2 &#34;zkServer.sh&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;zkServer.sh&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# zkServer.sh
JMX enabled by default
Using config: /usr/cx/zookeeper-3.4.6/bin/../conf/zoo.cfg
Usage: /usr/cx/zookeeper-3.4.6/bin/zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}
[root@realtime-1 ~]#
</pre><p>由输出内容可以看出，ZooKeeper环境变量已经配置正确，并且可以正常执行。</p><h2 id=7-zookeeper启动及状态查看>7 ZooKeeper启动及状态查看<a hidden class=anchor aria-hidden=true href=#7-zookeeper启动及状态查看>#</a></h2><h3 id=步骤1-zookeeper启动>步骤1. ZooKeeper启动<a hidden class=anchor aria-hidden=true href=#步骤1-zookeeper启动>#</a></h3><ol><li>通过下面的命令启动ZooKeeper服务：</li></ol><pre class=mermaid>zkServer.sh start
</pre><pre class=mermaid>ssh realtime-2 &#34;zkServer.sh start&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;zkServer.sh start&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# zkServer.sh start
JMX enabled by default
Using config: /usr/cx/zookeeper-3.4.6/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@realtime-1 ~]#
</pre><h3 id=步骤2-zookeeper运行状态查看>步骤2. ZooKeeper运行状态查看<a hidden class=anchor aria-hidden=true href=#步骤2-zookeeper运行状态查看>#</a></h3><p>ZooKeeper运行之后会随机进行follower角色以及leader角色选举，当leader角色节点出现异常后，会从其他节点中选举出新的leader角色。至于具体哪个节点处于leader状态，需要根据实际情况确定，并不是千篇一律的</p><p>通过下面的命令可以查看ZooKeeper运行状态：</p><pre class=mermaid>zkServer.sh status
</pre><pre class=mermaid>ssh realtime-2 &#34;zkServer.sh status&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;zkServer.sh status&#34;
</pre><p>命令运行后的返回结果如下所示（由返回结果的Mode字段可以看出，当前节点是作为follower角色运行的）：</p><pre class=mermaid>[root@realtime-1 ~]# zkServer.sh status
JMX enabled by default
Using config: /usr/cx/zookeeper-3.4.6/bin/../conf/zoo.cfg
Mode: follower
[root@realtime-1 ~]#
</pre><h2 id=8-安装配置hadoop集群>8 安装配置Hadoop集群<a hidden class=anchor aria-hidden=true href=#8-安装配置hadoop集群>#</a></h2><p>Hadoop需要在集群3个节点进行安装配置，为了操作方便，我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行</p><blockquote><p>在模板中，我们已经将相应的Hadoop安装包hadoop-2.7.1.tar.gz放到/usr/software/目录下，同学们不需要再次下载，可以直接使用。</p></blockquote><h3 id=步骤1-数据存储目录创建>步骤1. 数据存储目录创建<a hidden class=anchor aria-hidden=true href=#步骤1-数据存储目录创建>#</a></h3><pre class=mermaid>mkdir -p /hdfs/namenode
mkdir -p /hdfs/datanode
mkdir -p /hdfs/journalnode
mkdir -p /var/log/hadoop-yarn
ssh realtime-2 &#34;mkdir -p /hdfs/namenode&#34;
ssh realtime-2 &#34;mkdir -p /hdfs/datanode&#34;
ssh realtime-2 &#34;mkdir -p /hdfs/journalnode&#34;
ssh realtime-2 &#34;mkdir -p /var/log/hadoop-yarn&#34;
ssh realtime-3 &#34;mkdir -p /hdfs/namenode&#34;
ssh realtime-3 &#34;mkdir -p /hdfs/datanode&#34;
ssh realtime-3 &#34;mkdir -p /hdfs/journalnode&#34;
ssh realtime-3 &#34;mkdir -p /var/log/hadoop-yarn&#34;
</pre><h3 id=步骤2-解压安装文件>步骤2. 解压安装文件<a hidden class=anchor aria-hidden=true href=#步骤2-解压安装文件>#</a></h3><p>通过下列命令解压Hadoop安装文件，将文件解压到/usr/cx目录下：</p><pre class=mermaid>tar -zxvf /usr/software/hadoop-2.7.1.tar.gz -C /usr/cx
</pre><p>命令执行后的输出内容如下所示：</p><pre class=mermaid>(-------------------省略------------------------)
hadoop-2.7.1/libexec/hdfs-config.sh
hadoop-2.7.1/README.txt
hadoop-2.7.1/NOTICE.txt
hadoop-2.7.1/lib/
hadoop-2.7.1/lib/native/
hadoop-2.7.1/lib/native/libhadoop.a
hadoop-2.7.1/lib/native/libhadoop.so
hadoop-2.7.1/lib/native/libhadooppipes.a
hadoop-2.7.1/lib/native/libhdfs.so.0.0.0
hadoop-2.7.1/lib/native/libhadooputils.a
hadoop-2.7.1/lib/native/libhdfs.a
hadoop-2.7.1/lib/native/libhdfs.so
hadoop-2.7.1/lib/native/libhadoop.so.1.0.0
hadoop-2.7.1/LICENSE.txt
[root@master ~]#
</pre><h3 id=步骤3-编辑hadoop配置文件>步骤3. 编辑Hadoop配置文件：<a hidden class=anchor aria-hidden=true href=#步骤3-编辑hadoop配置文件>#</a></h3><ol><li>使用vi命令打开hadoop-env.sh配置文件进行编辑：</li></ol><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/hadoop-env.sh
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>(-------------------省略------------------------)
# Set Hadoop-specific environment variables here.

# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
export JAVA_HOME=${JAVA_HOME}             (注：需要对此行内容进行更改，为Hadoop绑定Java运行环境)

# The jsvc implementation to use. Jsvc is required to run secure datanodes
# that bind to privileged ports to provide authentication of data transfer
# protocol.  Jsvc is not required if SASL is configured for authentication of
# data transfer protocol using non-privileged ports.
#export JSVC_HOME=${JSVC_HOME}

export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-&#34;/etc/hadoop&#34;}
(-------------------省略------------------------)
</pre><p>在文件中进行下列内容更改，将JAVA_HOME对应的值改成实际的JDK安装路径：</p><p>export JAVA_HOME=/usr/cx/jdk1.8.0_60</p><p>编辑完成后保存文件并退出vi编辑器。</p><ol start=2><li>使用vi命令打开hdfs-site.xml文件进行配置：</li></ol><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/hdfs-site.xml
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;

&lt;!-- Put site-specific property overrides in this file. --&gt;

&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
&lt;/configuration&gt;
</pre><p>在文件中<code>&lt;configuration></code>和<code>&lt;/configuration></code>之间增加下列内容：</p><pre class=mermaid>/*配置DataNode的数据存储目录，需要与上文创建的目录相对应*/
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;/hdfs/datanode&lt;/value&gt;
&lt;/property&gt;
/*配置数据块大小为256M*/
&lt;property&gt;
&lt;name&gt;dfs.blocksize&lt;/name&gt;
&lt;value&gt;268435456&lt;/value&gt;
&lt;/property&gt;
/*自定义的HDFS服务名，在高可用集群中，无法配置单一HDFS服务器入口，所以需要指定一个逻辑上的服务名，当访问服务名时，会自动选择NameNode节点进行访问*/
&lt;property&gt;
&lt;name&gt;dfs.nameservices&lt;/name&gt;
&lt;value&gt;HDFScluster&lt;/value&gt;
&lt;/property&gt;
/*配置NameNode的数据存储目录，需要与上文创建的目录相对应*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;/hdfs/namenode&lt;/value&gt;
&lt;/property&gt;
/*定义HDFS服务名所指向的NameNode主机名称*/
&lt;property&gt;
&lt;name&gt;dfs.ha.namenodes.HDFScluster&lt;/name&gt;
&lt;value&gt;realtime-1,realtime-2&lt;/value&gt;
&lt;/property&gt;
/*设置NameNode的完整监听地址*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.rpc-address.HDFScluster.realtime-1&lt;/name&gt;
&lt;value&gt;realtime-1:8020&lt;/value&gt;
&lt;/property&gt;
/*设置NameNode的完整监听地址*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.rpc-address.HDFScluster.realtime-2&lt;/name&gt;
&lt;value&gt;realtime-2:8020&lt;/value&gt;
&lt;/property&gt;
/*设置NameNode的HTTP访问地址*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address.HDFScluster.realtime-1&lt;/name&gt;
&lt;value&gt;realtime-1:50070&lt;/value&gt;
&lt;/property&gt;
/*设置NameNode的HTTP访问地址*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address.HDFScluster.realtime-2&lt;/name&gt;
&lt;value&gt;realtime-2:50070&lt;/value&gt;
&lt;/property&gt;
/*设置主从NameNode元数据同步地址，官方推荐将nameservice作为最后的journal ID*/
&lt;property&gt;
&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;
&lt;value&gt;qjournal://realtime-1:8485;realtime-2:8485;realtime-3:8485/HDFScluster&lt;/value&gt;
&lt;/property&gt;
/*设置HDFS客户端用来连接集群中活动状态NameNode节点的Java类*/
&lt;property&gt;
&lt;name&gt;dfs.client.failover.proxy.provider.HDFScluster&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;
&lt;/property&gt;
/*设置SSH登录的私钥文件地址*/
&lt;property&gt;
&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;
&lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;
&lt;/property&gt;
/*启动fence过程，确保集群高可用性*/
&lt;property&gt;
&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
&lt;value&gt;shell(/bin/true)&lt;/value&gt;
&lt;/property&gt;
/*配置JournalNode的数据存储目录，需要与上文创建的目录相对应*/
&lt;property&gt;
&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
&lt;value&gt;/hdfs/journalnode&lt;/value&gt;
&lt;/property&gt;
/*设置自动切换活跃节点，保证集群高可用性*/
&lt;property&gt;
&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
/*配置数据块副本数*/
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;3&lt;/value&gt;
&lt;/property&gt;
/*将dfs.webhdfs.enabled属性设置为true，否则就不能使用webhdfs的LISTSTATUS、LIST FILESTATUS等需要列出文件、文件夹状态的命令，因为这些信息都是由namenode保存的*/
&lt;property&gt;
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
</pre><p>编辑完成后保存文件并退出vi编辑器</p><p><em>在集群中，对HDFS集群访问的入口是NameNode所在的服务器。但是在两个NameNode节点的HA集群中，无法配置单一服务器入口，所以需要通过dfs.nameservices指定一个逻辑上的服务名，这个服务名是自定义的。当外界访问HDFS集群时，入口就变为这个服务名称，Hadoop会自动实现将访问请求转发到实际的处于Active状态的NameNode节点上。</em></p><p><em>当配置了HDFS HA集群时，会有两个NameNode，为了避免两个NameNode都为Active状态，当发生failover时，Standby的节点要执行一系列方法把原来那个Active节点中不健康的NameNode服务给杀掉（这个过程就称为fence）。而dfs.ha.fencing.methods配置就是配置了执行杀死原来Active NameNode服务的方法，为了保险起见，因此指定无论如何都把StandBy节点的状态提升为Active，所以最后要配置一个shell(/bin/true)，保证不论前面的方法执行的情况如何，最后fence过程返回的结果都为True。fence操作需要通过SSH进行节点间的访问，因此需要配置dfs.ha.fencing.ssh.private-key-files为所需要用到的私钥文件路径信息。</em></p><ol start=3><li>使用vi命令打开core-site.xml配置文件进行编辑：</li></ol><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/core-site.xml
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;

&lt;!-- Put site-specific property overrides in this file. --&gt;

&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
&lt;/configuration&gt;
</pre><p>在文件中和之间增加下列内容：</p><pre class=mermaid>/*设置默认的HDFS访问路径，需要与hdfs-site.xml中的HDFS服务名相一致*/
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://HDFScluster&lt;/value&gt;
&lt;/property&gt;
/*临时文件夹路径设置*/
&lt;property&gt; 
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt; 
&lt;value&gt;/usr/tmp&lt;/value&gt; 
&lt;/property&gt; 
/*配置ZooKeeper服务集群，用于活跃NameNode节点的选举*/
&lt;property&gt;
&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
&lt;value&gt;realtime-1:2181,realtime-2:2181,realtime-3:2181&lt;/value&gt;
&lt;/property&gt;
/*设置数据压缩算法*/
&lt;property&gt;
&lt;name&gt;io.compression.codecs&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;
&lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;
&lt;/property&gt;
/*设置使用hduser用户可以代理所有主机用户进行任务提交*/
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.hduser.host&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
/*设置使用hduser用户可以代理所有组用户进行任务提交*/
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
</pre><p>编辑完成后保存文件并退出vi编辑器</p><p><em>对HDFS集群访问的入口是NameNode所在的服务器，但是在两个NameNode节点的HA集群中，无法配置单一服务器入口，所以需要在hdfs-site.xml中通过dfs.nameservices指定一个逻辑上的服务名，因此此处的fs.defaultFS配置的入口地址需要与hdfs-site.xml中dfs.nameservices所配置的一致。</em></p><ol start=4><li>使用vi命令打开yarn-site.xml文件进行配置：</li></ol><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/yarn-site.xml
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;
&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
&lt;!-- Site specific YARN configuration properties --&gt;

&lt;/configuration&gt;
</pre><p>在文件中和之间增加下列内容：</p><pre class=mermaid>/*设置NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
/*设置任务日志存储目录*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
&lt;value&gt;file:///var/log/hadoop-yarn &lt;/value&gt;
&lt;/property&gt;
/*设置Hadoop依赖包地址*/
&lt;property&gt;
&lt;name&gt;yarn.application.classpath&lt;/name&gt;
&lt;value&gt;
$HADOOP_HOME/share/hadoop/common/*,$HADOOP_HOME/share/hadoop/common/lib/*,
$HADOOP_HOME/share/hadoop/hdfs/*,$HADOOP_HOME/share/hadoop/hdfs/lib/*,
$HADOOP_HOME/share/hadoop/mapreduce/*,$HADOOP_HOME/share/hadoop/mapreduce/lib/*,
$HADOOP_HOME/share/hadoop/yarn/*,$HADOOP_HOME/share/hadoop/yarn/lib/*
&lt;/value&gt;
&lt;/property&gt;
/*开启resourcemanager 的高可用性功能*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
/*标识集群中的resourcemanager，如果设置该选项，需要确保所有的resourcemanager节点在配置中都有自己的逻辑id*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;
&lt;value&gt;YARNcluster&lt;/value&gt;
&lt;/property&gt;
/*设置resourcemanager节点的逻辑id*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;
&lt;value&gt;rm1,rm2&lt;/value&gt;
&lt;/property&gt;
/*为每个逻辑id绑定实际的主机名称*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;
&lt;value&gt;realtime-1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;
&lt;value&gt;realtime-2&lt;/value&gt;
&lt;/property&gt;
/*指定ZooKeeper服务地址*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
&lt;value&gt;realtime-1:2181,realtime-2:2181,realtime-3:2181&lt;/value&gt;
&lt;/property&gt;
/*指定resourcemanager的WEB访问地址*/
&lt;property&gt; 
&lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; 
&lt;value&gt;realtime-1:8089&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt; 
&lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; 
&lt;value&gt;realtime-2:8089&lt;/value&gt; 
&lt;/property&gt;
/*设定虚拟内存与实际内存的比例，比例值越高，则可用虚拟内存就越多*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
&lt;value&gt;3&lt;/value&gt;
&lt;/property&gt;
/*设定单个容器可以申领到的最小内存资源*/
&lt;property&gt;
&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
&lt;value&gt;32&lt;/value&gt;
&lt;/property&gt;
/*设置当任务运行结束后，日志文件被转移到的HDFS目录*/
&lt;property&gt; 
&lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
&lt;value&gt;hdfs://HDFScluster/var/log/hadoop-yarn/apps&lt;/value&gt;
&lt;/property&gt;
/*设定资源调度策略，目前可用的有FIFO、Capacity Scheduler和Fair Scheduler*/
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;
&lt;/property&gt;
/*设定每个任务能够申领到的最大虚拟CPU数目*/
&lt;property&gt; 
&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; 
&lt;value&gt;8&lt;/value&gt;
&lt;/property&gt;
/*设置任务完成指定时间（秒）之后，删除任务的本地化文件和日志目录*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.delete.debug-delay-sec&lt;/name&gt;
&lt;value&gt;600&lt;/value&gt;
&lt;/property&gt;
/*设置志在HDFS上保存多长时间（秒）*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.log.retain-seconds&lt;/name&gt;
&lt;value&gt;86400&lt;/value&gt;
&lt;/property&gt;
/*设定物理节点有2G内存加入资源池*/
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
&lt;value&gt;2048&lt;/value&gt;
&lt;/property&gt;
</pre><p>编辑完成后保存文件并退出vi编辑器</p><p><em>在集群中，提交任务的入口是ResourceManager所在的服务器。但是在两个ResourceManager节点的HA集群中，无法配置单一服务器入口，所以需要通过yarn.resourcemanager.cluster-id指定一个逻辑上的服务名，这个服务名是自定义的。当外界向集群提交任务时，入口就变为这个服务名称，YARN会自动实现将访问请求转发到实际的处于Active状态的ResourceManager节点上。由于配置了逻辑服务名，所以需要设置resourcemanager节点的逻辑id，并为每个逻辑id绑定实际的主机名称</em></p><ol start=5><li>使用下列命令复制mapred-site.xml.template文件并重命名为mapred-site.xml：</li></ol><pre class=mermaid>cp /usr/cx/hadoop-2.7.1/etc/hadoop/mapred-site.xml.template /usr/cx/hadoop-2.7.1/etc/hadoop/mapred-site.xml
</pre><ol start=6><li>使用vi命令打开mapred-site.xml文件进行配置：</li></ol><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/mapred-site.xml
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>&lt;?xml version=&#34;1.0&#34;?&gt;
&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;

&lt;!-- Put site-specific property overrides in this file. --&gt;

&lt;configuration&gt;
(注：需要在此处进行相关内容配置)
&lt;/configuration&gt;
</pre><p>在文件中和之间增加下列内容：</p><pre class=mermaid>/*Hadoop对MapReduce运行框架一共提供了3种实现，在mapred-site.xml中通过&#34;mapreduce.framework.name&#34;这个属性来设置为&#34;classic&#34;、&#34;yarn&#34;或者&#34;local&#34;*/
&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</pre><p>编辑完成后保存文件并退出vi编辑器</p><ol start=7><li>使用vi命令打开slaves文件进行配置（要与我们前文设置的主机名相互一致，否则将会引起Hadoop相关进程无法正确启动）：</li></ol><pre class=mermaid>vi /usr/cx/hadoop-2.7.1/etc/hadoop/slaves
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid>localhost        （注：需要对此内容进行更改，配置为Slave节点的实际主机名）
</pre><p>将文件中的内容更改为下列内容：</p><pre class=mermaid>realtime-1
realtime-2
realtime-3
</pre><p>编辑完成后保存文件并退出vi编辑器</p><h3 id=步骤4-文件分发>步骤4. 文件分发<a hidden class=anchor aria-hidden=true href=#步骤4-文件分发>#</a></h3><ol><li>通过下面的命令将节点1的Hadoop文件包分发到节点2中：</li></ol><pre class=mermaid>scp -r /usr/cx/hadoop-2.7.1 root@realtime-2:/usr/cx/
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>（---------------------省略-----------------------）
external.png                                  100%  230     0.2KB/s   00:00   
banner.jpg                                    100%  872     0.9KB/s   00:00   
maven-feather.png                             100% 3330     3.3KB/s   00:00   
build-by-maven-white.png                      100% 2260     2.2KB/s   00:00   
build-by-maven-black.png                      100% 2294     2.2KB/s   00:00   
bg.jpg                                        100%  486     0.5KB/s   00:00   
icon_error_sml.gif                            100% 1010     1.0KB/s   00:00   
logo_apache.jpg                               100%   33KB  32.7KB/s   00:00   
collapsed.gif                                 100%  820     0.8KB/s   00:00   
apache-maven-project-2.png                    100%   33KB  32.7KB/s   00:00   
icon_success_sml.gif                          100%  990     1.0KB/s   00:00   
icon_info_sml.gif                             100%  606     0.6KB/s   00:00   
h3.jpg                                        100%  431     0.4KB/s   00:00   
maven-logo-2.gif                              100%   26KB  25.8KB/s   00:00   
h5.jpg                                        100%  357     0.4KB/s   00:00   
newwindow.png                                 100%  220     0.2KB/s   00:00   
icon_warning_sml.gif                          100%  576     0.6KB/s   00:00   
expanded.gif                                  100%   52     0.1KB/s   00:00 
dependency-analysis.html                      100%   21KB  21.3KB/s   00:00   
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下面的命令将节点1的Hadoop文件包分发到节点3中：</li></ol><pre class=mermaid>scp -r /usr/cx/hadoop-2.7.1 root@realtime-3:/usr/cx/
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>（---------------------省略-----------------------）
external.png                                  100%  230     0.2KB/s   00:00   
banner.jpg                                    100%  872     0.9KB/s   00:00   
maven-feather.png                             100% 3330     3.3KB/s   00:00   
build-by-maven-white.png                      100% 2260     2.2KB/s   00:00   
build-by-maven-black.png                      100% 2294     2.2KB/s   00:00   
bg.jpg                                        100%  486     0.5KB/s   00:00   
icon_error_sml.gif                            100% 1010     1.0KB/s   00:00   
logo_apache.jpg                               100%   33KB  32.7KB/s   00:00   
collapsed.gif                                 100%  820     0.8KB/s   00:00   
apache-maven-project-2.png                    100%   33KB  32.7KB/s   00:00   
icon_success_sml.gif                          100%  990     1.0KB/s   00:00   
icon_info_sml.gif                             100%  606     0.6KB/s   00:00   
h3.jpg                                        100%  431     0.4KB/s   00:00   
maven-logo-2.gif                              100%   26KB  25.8KB/s   00:00   
h5.jpg                                        100%  357     0.4KB/s   00:00   
newwindow.png                                 100%  220     0.2KB/s   00:00   
icon_warning_sml.gif                          100%  576     0.6KB/s   00:00   
expanded.gif                                  100%   52     0.1KB/s   00:00   
dependency-analysis.html                      100%   21KB  21.3KB/s   00:00   
[root@realtime-1 ~]#
</pre><h3 id=步骤5-配置hadoop环境变量>步骤5. 配置Hadoop环境变量<a hidden class=anchor aria-hidden=true href=#步骤5-配置hadoop环境变量>#</a></h3><ol><li>通过下列命令使用vi编辑器编辑~/.bashrc文件：</li></ol><pre class=mermaid>vi  ~/.bashrc
</pre><p>打开后的文件内容如下所示：</p><pre class=mermaid># .bashrc

# User specific aliases and functions

alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
export JAVA_HOME=/usr/cx/jdk1.8.0_60
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/tools.jar
export ZK_HOME=/usr/cx/zookeeper-3.4.6
export PATH=$PATH:$ZK_HOME/bin
(----------------在此处增加内容-------------------)
</pre><ol start=2><li>在~/.bashrc文件中增加以下内容：</li></ol><pre class=mermaid>export HADOOP_HOME=/usr/cx/hadoop-2.7.1
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
</pre><p>编辑完成后保存文件并退出vi编辑器</p><ol start=3><li>通过下面的命令将节点1的环境变量文件分发到节点2中：</li></ol><pre class=mermaid>scp -r ~/.bashrc root@realtime-2:~/.bashrc
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# scp -r ~/.bashrc root@realtime-2:~/.bashrc
.bashrc                                       100%  502     0.5KB/s   00:00   
[root@realtime-1 ~]#
</pre><ol start=4><li>通过下面的命令将节点1的环境变量文件分发到节点3中：</li></ol><pre class=mermaid>scp -r ~/.bashrc root@realtime-3:~/.bashrc
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# scp -r ~/.bashrc root@realtime-3:~/.bashrc
.bashrc                                       100%  502     0.5KB/s   00:00   
[root@realtime-1 ~]#
</pre><h3 id=步骤6-更新环境变量>步骤6. 更新环境变量<a hidden class=anchor aria-hidden=true href=#步骤6-更新环境变量>#</a></h3><ol><li>执行如下命令，更新环境变量：</li></ol><pre class=mermaid>source  ~/.bashrc
</pre><pre class=mermaid>ssh realtime-2 &#34;source  ~/.bashrc&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;source  ~/.bashrc&#34;
</pre><h3 id=步骤7-验证hadoop环境变量是否配置成功>步骤7. 验证Hadoop环境变量是否配置成功<a hidden class=anchor aria-hidden=true href=#步骤7-验证hadoop环境变量是否配置成功>#</a></h3><ol><li>通过下列命令验证Hadoop环境变量是否配置成功：</li></ol><pre class=mermaid>hadoop
</pre><pre class=mermaid>ssh realtime-2 &#34;hadoop&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;hadoop&#34;
</pre><pre class=mermaid>如果出现如下提示信息，则说明Hadoop安装配置成功：
</pre><pre class=mermaid>[root@realtime-1 ~]# hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar &lt;jar&gt;            run a jar file
                       note: please use &#34;yarn jar&#34; to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively
  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.
[root@realtime-1 ~]#
</pre><p><em>如果没有正确输出相关信息，请检查~/.bashrc文件中的Hadoop环境变量是否配置正确，同时请确定是否使用source ~/.bashrc命令更新环境变量配置</em></p><h3 id=步骤8-格式化hdfs>步骤8. 格式化HDFS<a hidden class=anchor aria-hidden=true href=#步骤8-格式化hdfs>#</a></h3><p>通过下列命令格式化HDFS文件系统（如果格式化失败，会有相关的错误日志输出，根据输出内容进行更改即可）：</p><pre class=mermaid>hadoop namenode -format
</pre><p>命令运行后的部分显示内容如下所示：</p><pre class=mermaid>(-------------------省略------------------------)
18/11/30 11:07:15 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
18/11/30 11:07:15 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
18/11/30 11:07:15 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
18/11/30 11:07:15 INFO util.GSet: Computing capacity for map NameNodeRetryCache
18/11/30 11:07:15 INFO util.GSet: VM type       = 64-bit
18/11/30 11:07:15 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
18/11/30 11:07:15 INFO util.GSet: capacity      = 2^15 = 32768 entries
18/11/30 11:07:16 INFO namenode.FSImage: Allocated new BlockPoolId: BP-348760827-10.1.1.4-1543547236332
18/11/30 11:07:16 INFO common.Storage: Storage directory /hdfs/namenode has been successfully formatted.
18/11/30 11:07:16 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0
18/11/30 11:07:16 INFO util.ExitUtil: Exiting with status 0
18/11/30 11:07:16 INFO namenode.NameNode: SHUTDOWN_MSG:

/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at realtime-1/10.1.1.4
************************************************************/

[root@realtime-1 ~]#
</pre><h3 id=步骤9-格式化zkfc元数据>步骤9. 格式化zkfc元数据<a hidden class=anchor aria-hidden=true href=#步骤9-格式化zkfc元数据>#</a></h3><p>通过下面的命令格式化DFSZKFailoverController(ZKFC)元数据（在一个节点中进行处理即可）：</p><pre class=mermaid>hdfs zkfc -formatZK
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>（---------------省略------------------）
tString=realtime-1:2181,realtime-2:2181,realtime-3:2181 sessionTimeout=5000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@10e31a9a
18/11/30 11:37:46 INFO zookeeper.ClientCnxn: Opening socket connection to server realtime-2/10.1.1.3:2181. Will not attempt to authenticate using SASL (unknown error)
18/11/30 11:37:47 INFO zookeeper.ClientCnxn: Socket connection established to realtime-2/10.1.1.3:2181, initiating session
18/11/30 11:37:47 INFO zookeeper.ClientCnxn: Session establishment complete on server realtime-2/10.1.1.3:2181, sessionid = 0x2675e9a37a90000, negotiated timeout = 5000
18/11/30 11:37:47 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/HDFScluster in ZK.
18/11/30 11:37:47 INFO ha.ActiveStandbyElector: Session connected.
18/11/30 11:37:47 INFO zookeeper.ZooKeeper: Session: 0x2675e9a37a90000 closed
18/11/30 11:37:47 INFO zookeeper.ClientCnxn: EventThread shut down
[root@realtime-1 ~]#
</pre><h2 id=9-hadoop集群启动运行>9 Hadoop集群启动运行<a hidden class=anchor aria-hidden=true href=#9-hadoop集群启动运行>#</a></h2><p>我们在节点1进行下列操作，在节点1中通过ssh远程登录到节点2和节点3中，实现命令的分发与运行</p><h3 id=步骤1-启动hdfs相关服务>步骤1. 启动HDFS相关服务<a hidden class=anchor aria-hidden=true href=#步骤1-启动hdfs相关服务>#</a></h3><ol><li>通过下面的命令可以启动HDFS相关服务：</li></ol><pre class=mermaid>start-dfs.sh
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# start-dfs.sh
18/11/30 11:55:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [realtime-2 realtime-1]
realtime-2: starting namenode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-namenode-realtime-2.out
realtime-1: starting namenode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-namenode-realtime-1.out
realtime-1: starting datanode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-datanode-realtime-1.out
realtime-2: starting datanode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-datanode-realtime-2.out
realtime-3: starting datanode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-datanode-realtime-3.out
Starting journal nodes [realtime-1 realtime-2 realtime-3]
realtime-1: starting journalnode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-journalnode-realtime-1.out
realtime-2: starting journalnode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-journalnode-realtime-2.out
realtime-3: starting journalnode, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-journalnode-realtime-3.out
18/11/30 11:56:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting ZK Failover Controllers on NN hosts [realtime-2 realtime-1]
realtime-2: starting zkfc, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-zkfc-realtime-2.out
realtime-1: starting zkfc, logging to /usr/cx/hadoop-2.7.1/logs/hadoop-root-zkfc-realtime-1.out
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下面的命令查看节点1中对应的相关服务：</li></ol><pre class=mermaid>jps
</pre><pre class=mermaid>ssh realtime-2 &#34;jps&#34;
</pre><pre class=mermaid>ssh realtime-3 &#34;jps&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# jps
10033 ResourceManager
9427 DataNode
9315 NameNode
2597 QuorumPeerMain
10457 Jps
9625 JournalNode
9818 DFSZKFailoverController
10140 NodeManager
1743 VmServer.jar
[root@realtime-1 ~]#
</pre><p>通过下面的命令在节点2中启动ResourceManager进程：</p><pre class=mermaid>ssh realtime-2 &#34;yarn-daemon.sh start resourcemanager&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-2 &#34;yarn-daemon.sh start resourcemanager&#34;
starting resourcemanager, logging to /usr/cx/hadoop-2.7.1/logs/yarn-root-resourcemanager-realtime-2.out
[root@realtime-1 ~]#
</pre><p>通过下面的命令查看节点2中对应的相关服务：</p><pre class=mermaid>ssh realtime-2 &#34;jps&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-2 &#34;jps&#34;
5792 DataNode
6164 NameNode
1703 VmServer.jar
6779 ResourceManager
6428 NodeManager
5981 DFSZKFailoverController
6846 Jps
2686 QuorumPeerMain
5887 JournalNode
[root@realtime-1 ~]#
</pre><p>由返回结果可以看出，此时在节点2中已经成功启动了ResourceManager进程</p><h2 id=10-hadoop-高可用性测试>10 Hadoop 高可用性测试<a hidden class=anchor aria-hidden=true href=#10-hadoop-高可用性测试>#</a></h2><p>笔者在写作过程中是在节点1中进行下面的操作，同学们可以在任意节点中进行下面的操作，所实现的效果是一致的</p><h3 id=步骤1-nodemanager状态查看>步骤1. NodeManager状态查看<a hidden class=anchor aria-hidden=true href=#步骤1-nodemanager状态查看>#</a></h3><p>由于设置了2个NameNode，因此必然会有一个处于Active状态，一个处于StandBy状态，至于具体哪个节点处于Active状态，需要根据实际情况确定，并不是千篇一律的。</p><ol><li>当Hadoop成功启动后，我们打开浏览器，输入网址http://realtime-1:50070便可以访问HDFS的Web管理页面（此时可以看到realtime-1节点是处于active状态的）：</li></ol><p><img alt=pab7rz loading=lazy src=https://qiniu.waite.wang/pab7rz.png></p><ol start=2><li>输入网址http://realtime-2:50070依然可以访问HDFS的Web管理页面（此时可以看到realtime-2节点是处于standby状态的）：</li></ol><p><img alt=noepa4 loading=lazy src=https://qiniu.waite.wang/noepa4.png></p><h3 id=步骤2-resourcemanager状态查看>步骤2. ResourceManager状态查看<a hidden class=anchor aria-hidden=true href=#步骤2-resourcemanager状态查看>#</a></h3><p>由于设置了2个ResourceManager，因此必然会有一个处于Active状态，一个处于StandBy状态，至于具体哪个节点处于Active状态，需要根据实际情况确定，并不是千篇一律的。</p><ol><li>在终端模拟器中，通过下面的命令可以查看逻辑ID为rm1（实际映射的节点为realtime-1）的节点对应的ResourceManager状态：</li></ol><pre class=mermaid>yarn rmadmin -getServiceState rm1
</pre><p>命令运行后的返回结果如下所示（可见当前节点是active状态）：</p><pre class=mermaid>[root@realtime-1 ~]# yarn rmadmin -getServiceState rm1
18/11/30 16:35:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
active
[root@realtime-1 ~]#
</pre><ol start=2><li>在终端模拟器中，通过下面的命令可以查看逻辑ID为rm2（实际映射的节点为realtime-2）的节点对应的ResourceManager状态：</li></ol><pre class=mermaid>yarn rmadmin -getServiceState rm2
</pre><p>命令运行后的返回结果如下所示（可见当前节点是standby状态）：</p><pre class=mermaid>[root@realtime-1 ~]# yarn rmadmin -getServiceState rm2
18/11/30 16:35:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
standby
[root@realtime-1 ~]#
</pre><h3 id=步骤3-hdfs高可用测试>步骤3. HDFS高可用测试<a hidden class=anchor aria-hidden=true href=#步骤3-hdfs高可用测试>#</a></h3><ol><li>通过下面的命令在HDFS中创建测试文件夹/test：</li></ol><pre class=mermaid>hadoop fs -mkdir /test
</pre><ol start=2><li>通过下面的命令查看HDFS中创建的测试文件夹/test：</li></ol><pre class=mermaid>hadoop fs -ls /
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# hadoop fs -ls /
18/11/30 16:40:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
drwxr-xr-x   - root supergroup          0 2018-11-30 16:39 /test
[root@realtime-1 ~]#
</pre><p>由返回结果可以看出，此时依然可以成功查询HDFS文件信息</p><p>打开浏览器，输入网址http://realtime-2:50070访问HDFS的Web管理页面，此时可以看到realtime-2节点已经成功接替成为NameNode并处于active状态（同学们需要根据实际情况来确定）：</p><p><img alt=lu3bmj loading=lazy src=https://qiniu.waite.wang/lu3bmj.png></p><h3 id=步骤4-yarn高可用测试>步骤4. YARN高可用测试<a hidden class=anchor aria-hidden=true href=#步骤4-yarn高可用测试>#</a></h3><ol><li>通过下面的命令，使用Hadoop自带的案例测试MapReduce应用程序的运行：</li></ol><pre class=mermaid>hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output
18/11/30 16:47:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/11/30 16:47:15 INFO input.FileInputFormat: Total input paths to process : 0
18/11/30 16:47:15 INFO mapreduce.JobSubmitter: number of splits:0
18/11/30 16:47:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1543557487449_0001
18/11/30 16:47:16 INFO impl.YarnClientImpl: Submitted application application_1543557487449_0001
18/11/30 16:47:16 INFO mapreduce.Job: The url to track the job: http://realtime-1:8089/proxy/application_1543557487449_0001/
18/11/30 16:47:16 INFO mapreduce.Job: Running job: job_1543557487449_0001
18/11/30 16:47:26 INFO mapreduce.Job: Job job_1543557487449_0001 running in uber mode : false
18/11/30 16:47:26 INFO mapreduce.Job:  map 0% reduce 0%
18/11/30 16:47:37 INFO mapreduce.Job:  map 0% reduce 100%
18/11/30 16:47:38 INFO mapreduce.Job: Job job_1543557487449_0001 completed successfully
18/11/30 16:47:39 INFO mapreduce.Job: Counters: 38
   File System Counters
        FILE: Number of bytes read=0
        FILE: Number of bytes written=119357
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=0
        HDFS: Number of bytes written=0
        HDFS: Number of read operations=3
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
   Job Counters
        Launched reduce tasks=1
        Total time spent by all maps in occupied slots (ms)=0
        Total time spent by all reduces in occupied slots (ms)=227232
        Total time spent by all reduce tasks (ms)=7101
        Total vcore-seconds taken by all reduce tasks=7101
        Total megabyte-seconds taken by all reduce tasks=7271424
   Map-Reduce Framework
        Combine input records=0
        Combine output records=0
        Reduce input groups=0
        Reduce shuffle bytes=0
        Reduce input records=0
        Reduce output records=0
        Spilled Records=0
        Shuffled Maps =0
        Failed Shuffles=0
        Merged Map outputs=0
        GC time elapsed (ms)=67
        CPU time spent (ms)=290
        Physical memory (bytes) snapshot=94629888
        Virtual memory (bytes) snapshot=2064699392
        Total committed heap usage (bytes)=30474240
   Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
   File Output Format Counters
        Bytes Written=0
[root@realtime-1 ~]#
</pre><ol start=2><li>通过下面的命令停止Active状态节点对应的ResourceManager进程（笔者写作过程中对应的为realtime-1节点，同学们需要根据实际情况来确定）</li></ol><pre class=mermaid>ssh realtime-1 &#34;yarn-daemon.sh stop resourcemanager&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-1 &#34;yarn-daemon.sh stop resourcemanager&#34;
stopping resourcemanager
[root@realtime-1 ~]#
</pre><ol start=3><li>通过下面的命令查看对应节点的进程信息：</li></ol><pre class=mermaid>ssh realtime-1 &#34;jps&#34;
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# ssh realtime-1 &#34;jps&#34;
9427 DataNode
2597 QuorumPeerMain
9625 JournalNode
9818 DFSZKFailoverController
10140 NodeManager
11885 Jps
1743 VmServer.jar
[root@realtime-1 ~]#
</pre><p>由返回结果可以看出，ResourceManager进程已经被停止</p><ol start=4><li>通过下面的命令，再次使用Hadoop自带的案例测试MapReduce应用程序的运行：</li></ol><pre class=mermaid>hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output1
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# hadoop jar /usr/cx/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /output1
18/11/30 16:50:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/11/30 16:50:31 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2
18/11/30 16:50:32 INFO input.FileInputFormat: Total input paths to process : 0
18/11/30 16:50:32 INFO mapreduce.JobSubmitter: number of splits:0
18/11/30 16:50:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1543567750404_0001
18/11/30 16:50:33 INFO impl.YarnClientImpl: Submitted application application_1543567750404_0001
18/11/30 16:50:33 INFO mapreduce.Job: The url to track the job: http://realtime-2:8089/proxy/application_1543567750404_0001/
18/11/30 16:50:33 INFO mapreduce.Job: Running job: job_1543567750404_0001
18/11/30 16:50:45 INFO mapreduce.Job: Job job_1543567750404_0001 running in uber mode : false
18/11/30 16:50:45 INFO mapreduce.Job:  map 0% reduce 0%
18/11/30 16:50:53 INFO mapreduce.Job:  map 0% reduce 100%
18/11/30 16:50:54 INFO mapreduce.Job: Job job_1543567750404_0001 completed successfully
18/11/30 16:50:54 INFO mapreduce.Job: Counters: 38
   File System Counters
        FILE: Number of bytes read=0
        FILE: Number of bytes written=119358
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=0
        HDFS: Number of bytes written=0
        HDFS: Number of read operations=3
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
   Job Counters
        Launched reduce tasks=1
        Total time spent by all maps in occupied slots (ms)=0
        Total time spent by all reduces in occupied slots (ms)=147936
        Total time spent by all reduce tasks (ms)=4623
        Total vcore-seconds taken by all reduce tasks=4623
        Total megabyte-seconds taken by all reduce tasks=4733952
   Map-Reduce Framework
        Combine input records=0
        Combine output records=0
        Reduce input groups=0
        Reduce shuffle bytes=0
        Reduce input records=0
        Reduce output records=0
        Spilled Records=0
        Shuffled Maps =0
        Failed Shuffles=0
        Merged Map outputs=0
        GC time elapsed (ms)=81
        CPU time spent (ms)=280
        Physical memory (bytes) snapshot=94146560
        Virtual memory (bytes) snapshot=2064695296
        Total committed heap usage (bytes)=30474240
   Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
   File Output Format Counters
        Bytes Written=0
[root@realtime-1 ~]#
</pre><p>由返回结果可以看出，此时YARN依然可以可靠的实现任务的调度</p><ol start=5><li>在终端模拟器中，通过下面的命令可以查看逻辑ID为rm2（实际映射的节点为realtime-2）的节点对应的ResourceManager状态（同学们需要根据实际情况来确定）：</li></ol><pre class=mermaid>yarn rmadmin -getServiceState rm2
</pre><p>命令运行后的返回结果如下所示：</p><pre class=mermaid>[root@realtime-1 ~]# yarn rmadmin -getServiceState rm2
18/11/30 16:51:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
active
[root@realtime-1 ~]#
</pre><p>由返回结果可以看出，当前节点已经自动成功接替变成了active状态</p><p><a href=http://49.234.55.187:8090/archives/hadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E4%B8%8E%E9%85%8D%E7%BD%AE target=_blank></a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://waite.wang/tags/hadoop/>Hadoop</a></li></ul><nav class=paginav><a class=prev href=https://waite.wang/posts/win/tomcat-idea-install/><span class=title>« 上一页</span><br><span>Tomcat + Idea 配置及使用</span>
</a><a class=next href=https://waite.wang/posts/bigdata/hadoop-group-install-and-config/><span class=title>下一页 »</span><br><span>hadoop集群的安装与配置</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Waite0603/HugoBlog issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span><a href=https://beian.miit.gov.cn/ target=_blank rel="noopener noreferrer">粤ICP备2022028437号-1</a></span><br><span>Copyright &copy; 2018 - 2025 By <a href=https://waite.wang/>Waite</a>, All Rights
Reserved.</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script src=/js/jquery-3.5.1.min.js></script><link rel=stylesheet href=/css/jquery.fancybox.min.css><script src=/js/jquery.fancybox.min.js></script></body></html>